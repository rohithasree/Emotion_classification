{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "adaffcea97324b589bd445a1bbab3f10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a44b048586b94eb6a76169235b74782a",
              "IPY_MODEL_ce1172eafbb6441e8ec1d9a221d86178",
              "IPY_MODEL_030a4c83d60e4dd6b3ad8ef7d747544c"
            ],
            "layout": "IPY_MODEL_f2edddf29f36441b86a08ee10b968929"
          }
        },
        "a44b048586b94eb6a76169235b74782a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce857b0176674e3cbc750a723fe4129f",
            "placeholder": "​",
            "style": "IPY_MODEL_4fc0ee947a474cc6ae76c0732ebb0572",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "ce1172eafbb6441e8ec1d9a221d86178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8f1e34436154ce8b0e852d17d07e088",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d1dd21de049e4f65920bc58e98841549",
            "value": 231508
          }
        },
        "030a4c83d60e4dd6b3ad8ef7d747544c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4248add1db994b71bdcfbb196a9355e5",
            "placeholder": "​",
            "style": "IPY_MODEL_d61cd0792fb94c9092900248ae7475ae",
            "value": " 232k/232k [00:00&lt;00:00, 1.12MB/s]"
          }
        },
        "f2edddf29f36441b86a08ee10b968929": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce857b0176674e3cbc750a723fe4129f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fc0ee947a474cc6ae76c0732ebb0572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8f1e34436154ce8b0e852d17d07e088": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1dd21de049e4f65920bc58e98841549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4248add1db994b71bdcfbb196a9355e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d61cd0792fb94c9092900248ae7475ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16d7c92d9a7d414ead73af526cf60746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_392898fe88b8404b8736d74576b98081",
              "IPY_MODEL_d17fae9256d84331b8dcadca11935dbf",
              "IPY_MODEL_784da13a9524475d9b99860026b51451"
            ],
            "layout": "IPY_MODEL_39fa392b43714f49a8eff3855874af02"
          }
        },
        "392898fe88b8404b8736d74576b98081": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a5cce6cb8664a1582cc4db9f612a048",
            "placeholder": "​",
            "style": "IPY_MODEL_ac91e565b99348769a66bb67f1b88b6c",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "d17fae9256d84331b8dcadca11935dbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf6dbea5f41d466094ee2d517c6f7552",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce8933f1ea244ad2b159982ce08efaae",
            "value": 28
          }
        },
        "784da13a9524475d9b99860026b51451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a9666140a79455abeec266ac3e3993f",
            "placeholder": "​",
            "style": "IPY_MODEL_32baab1a11154ce69922d74ce885bfbf",
            "value": " 28.0/28.0 [00:00&lt;00:00, 657B/s]"
          }
        },
        "39fa392b43714f49a8eff3855874af02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a5cce6cb8664a1582cc4db9f612a048": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac91e565b99348769a66bb67f1b88b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf6dbea5f41d466094ee2d517c6f7552": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce8933f1ea244ad2b159982ce08efaae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a9666140a79455abeec266ac3e3993f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32baab1a11154ce69922d74ce885bfbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c37b7109c8284b0595f6b83717336dbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9616c48a1124bfead8b02cd181a0030",
              "IPY_MODEL_b14c8cc7a2c94d15ac2688ea7717e578",
              "IPY_MODEL_8075b0fb90364788a8d1234b682f7fd7"
            ],
            "layout": "IPY_MODEL_bff11d95501d4911b7642ddf2bf74313"
          }
        },
        "e9616c48a1124bfead8b02cd181a0030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f8821b436f3418b95f79a63fc1cb78d",
            "placeholder": "​",
            "style": "IPY_MODEL_bd202bc07e3d43988eb55bffca0a675c",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "b14c8cc7a2c94d15ac2688ea7717e578": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08527858e6a4429ab8e60b64de7fb0e5",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d66222dee4944df3a2c0a3181afec35b",
            "value": 570
          }
        },
        "8075b0fb90364788a8d1234b682f7fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9183b0e712ce4062af3483599ec376b9",
            "placeholder": "​",
            "style": "IPY_MODEL_a6e827a696a94576b25e3360761a231d",
            "value": " 570/570 [00:00&lt;00:00, 11.8kB/s]"
          }
        },
        "bff11d95501d4911b7642ddf2bf74313": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f8821b436f3418b95f79a63fc1cb78d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd202bc07e3d43988eb55bffca0a675c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08527858e6a4429ab8e60b64de7fb0e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d66222dee4944df3a2c0a3181afec35b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9183b0e712ce4062af3483599ec376b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6e827a696a94576b25e3360761a231d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e6e73b934ec41d196afc76572ddfdca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_450bae7f3406478595f13d3f140bc1d5",
              "IPY_MODEL_5476ac654fc64834a93e630ae25f1dab",
              "IPY_MODEL_c0e26ab765e64d52b8ac87d39498a4a4"
            ],
            "layout": "IPY_MODEL_1e9ff4e452234deb829353c20746ddfd"
          }
        },
        "450bae7f3406478595f13d3f140bc1d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10ced279f44f47d6bb87f5c462c0ea0c",
            "placeholder": "​",
            "style": "IPY_MODEL_18ca6ae27d1445d994ab459e9734953a",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "5476ac654fc64834a93e630ae25f1dab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8ec69ad64954bea93bcf1a2af04dbcf",
            "max": 507,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a589863a436454a8bc4b258dda4d024",
            "value": 507
          }
        },
        "c0e26ab765e64d52b8ac87d39498a4a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90ad78be101b4799895c93a4d4f45054",
            "placeholder": "​",
            "style": "IPY_MODEL_b1733b2753414e6b8dd881948f9045cb",
            "value": " 507/507 [00:00&lt;00:00, 19.6kB/s]"
          }
        },
        "1e9ff4e452234deb829353c20746ddfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10ced279f44f47d6bb87f5c462c0ea0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18ca6ae27d1445d994ab459e9734953a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8ec69ad64954bea93bcf1a2af04dbcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a589863a436454a8bc4b258dda4d024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90ad78be101b4799895c93a4d4f45054": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1733b2753414e6b8dd881948f9045cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b6f61623dbf40fcaae63c017a79b140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48f3e38c0cab4385ac14649412cbddea",
              "IPY_MODEL_5b26cf5b8b804738ad7a4717dcd02ff6",
              "IPY_MODEL_e95bccef365b465fbb28736b060fc3f9"
            ],
            "layout": "IPY_MODEL_bae4de9d7c5547d7ac8ddc76c1031637"
          }
        },
        "48f3e38c0cab4385ac14649412cbddea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3812630c3c5c46abbb55a441b9494cdc",
            "placeholder": "​",
            "style": "IPY_MODEL_7d6f447b8fb2463c8dcbdd5884e93486",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "5b26cf5b8b804738ad7a4717dcd02ff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a3b695e7e404284a97406872187181a",
            "max": 134982446,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa85df07d1774423b77a8e36b37f4408",
            "value": 134982446
          }
        },
        "e95bccef365b465fbb28736b060fc3f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2acb6900c9984c0c95ea7e2cbb83a196",
            "placeholder": "​",
            "style": "IPY_MODEL_e60999a69c6c42b9a83c1047395741c6",
            "value": " 135M/135M [00:08&lt;00:00, 18.8MB/s]"
          }
        },
        "bae4de9d7c5547d7ac8ddc76c1031637": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3812630c3c5c46abbb55a441b9494cdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d6f447b8fb2463c8dcbdd5884e93486": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a3b695e7e404284a97406872187181a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa85df07d1774423b77a8e36b37f4408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2acb6900c9984c0c95ea7e2cbb83a196": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e60999a69c6c42b9a83c1047395741c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Yxg7QVdIrOp",
        "outputId": "dc6cf387-4ec2-4dba-c143-80dcc780530b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "import time\n",
        "import datetime\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score"
      ],
      "metadata": {
        "id": "GP1bR69XC4B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    # torch.cuda.set_device(0)\n",
        "    device = torch.device('cuda')\n",
        "    print('Using GPU: ', torch.cuda.current_device())\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF1IcM60DWX6",
        "outputId": "17fade20-1aae-48b5-ac4b-d1a002548232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU:  0\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import string"
      ],
      "metadata": {
        "id": "6PpRDYcZD011"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Bhaav-Dataset.csv',sep=',')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "iimLvpqoD4QB",
        "outputId": "8b1b4e22-13e3-4e15-b836-ff35dbcac2c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Sentences  Annotation\n",
              "0               रमजान के पूरे तीस रोजों के बाद ईद आयी है           1\n",
              "1                   कितना मनोहर, कितना सुहावना प्रभाव है           1\n",
              "2      वृक्षों पर अजीब हरियाली है, खेतों में कुछ अजीब...           1\n",
              "3      आज का सूर्य देखो, कितना प्यारा, कितना शीतल है,...           1\n",
              "4                                 गाँव में कितनी हलचल है           1\n",
              "...                                                  ...         ...\n",
              "20299                          फिर यहाँ सर्दी हो जाती है           4\n",
              "20300  दिन-भर मैं यह देखती रहती हूँ कि धूप का टुकड़ा क...           4\n",
              "20301  पार्क का कोई ऐसा कोना नहीं, जहाँ मैं घड़ी-आधा घ...           4\n",
              "20302              लेकिन यह बेंच मुझे सबसे अच्छी लगती है           1\n",
              "20303  एक तो इस पर पत्ते नहीं झरते और दूसरे... अरे, आ...           4\n",
              "\n",
              "[20304 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-20f2d2bd-5823-4ad0-9e7b-c50aa55761d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentences</th>\n",
              "      <th>Annotation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>रमजान के पूरे तीस रोजों के बाद ईद आयी है</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>कितना मनोहर, कितना सुहावना प्रभाव है</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>वृक्षों पर अजीब हरियाली है, खेतों में कुछ अजीब...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>आज का सूर्य देखो, कितना प्यारा, कितना शीतल है,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>गाँव में कितनी हलचल है</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20299</th>\n",
              "      <td>फिर यहाँ सर्दी हो जाती है</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20300</th>\n",
              "      <td>दिन-भर मैं यह देखती रहती हूँ कि धूप का टुकड़ा क...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20301</th>\n",
              "      <td>पार्क का कोई ऐसा कोना नहीं, जहाँ मैं घड़ी-आधा घ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20302</th>\n",
              "      <td>लेकिन यह बेंच मुझे सबसे अच्छी लगती है</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20303</th>\n",
              "      <td>एक तो इस पर पत्ते नहीं झरते और दूसरे... अरे, आ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20304 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-20f2d2bd-5823-4ad0-9e7b-c50aa55761d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-20f2d2bd-5823-4ad0-9e7b-c50aa55761d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-20f2d2bd-5823-4ad0-9e7b-c50aa55761d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.assign(joy=0)\n",
        "df2 = df1.assign(anger=0)\n",
        "df3 = df2.assign(suspense=0)\n",
        "df4 = df3.assign(sad=0)\n",
        "train_df = df4.assign(neutral=0)"
      ],
      "metadata": {
        "id": "3ArC9KV_EkK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index in train_df.index:\n",
        "    k = train_df['Annotation'][index]\n",
        "    if k == 0:\n",
        "        train_df['joy'][index] = 0\n",
        "        train_df['anger'][index] = 1\n",
        "        train_df['suspense'][index] = 0\n",
        "        train_df['sad'][index] = 0\n",
        "        train_df['neutral'][index] = 0\n",
        "    elif k == 1:\n",
        "        train_df['joy'][index] = 1\n",
        "        train_df['anger'][index] = 0\n",
        "        train_df['suspense'][index] = 0\n",
        "        train_df['sad'][index] = 0\n",
        "        train_df['neutral'][index] = 0\n",
        "    elif k == 2:\n",
        "        train_df['joy'][index] = 0\n",
        "        train_df['anger'][index] = 0\n",
        "        train_df['suspense'][index] = 0\n",
        "        train_df['sad'][index] = 1\n",
        "        train_df['neutral'][index] = 0\n",
        "    elif k == 3:\n",
        "        train_df['joy'][index] = 0\n",
        "        train_df['anger'][index] = 0\n",
        "        train_df['suspense'][index] = 1\n",
        "        train_df['sad'][index] = 0\n",
        "        train_df['neutral'][index] = 0\n",
        "    elif k==4:\n",
        "        train_df['joy'][index] = 0\n",
        "        train_df['anger'][index] = 0\n",
        "        train_df['suspense'][index] = 0\n",
        "        train_df['sad'][index] = 0\n",
        "        train_df['neutral'][index] = 1\n",
        "train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YqSBv4S9EqeX",
        "outputId": "49bd29fb-ff6c-4530-ec28-f6fa55b833e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-cee5a35a340a>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['joy'][index] = 1\n",
            "<ipython-input-7-cee5a35a340a>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['anger'][index] = 0\n",
            "<ipython-input-7-cee5a35a340a>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['suspense'][index] = 0\n",
            "<ipython-input-7-cee5a35a340a>:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['sad'][index] = 0\n",
            "<ipython-input-7-cee5a35a340a>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['neutral'][index] = 0\n",
            "<ipython-input-7-cee5a35a340a>:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['joy'][index] = 0\n",
            "<ipython-input-7-cee5a35a340a>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['anger'][index] = 0\n",
            "<ipython-input-7-cee5a35a340a>:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['suspense'][index] = 0\n",
            "<ipython-input-7-cee5a35a340a>:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['sad'][index] = 0\n",
            "<ipython-input-7-cee5a35a340a>:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['neutral'][index] = 1\n",
            "<ipython-input-7-cee5a35a340a>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['joy'][index] = 0\n",
            "<ipython-input-7-cee5a35a340a>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['anger'][index] = 1\n",
            "<ipython-input-7-cee5a35a340a>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['suspense'][index] = 0\n",
            "<ipython-input-7-cee5a35a340a>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['sad'][index] = 0\n",
            "<ipython-input-7-cee5a35a340a>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['neutral'][index] = 0\n",
            "<ipython-input-7-cee5a35a340a>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['joy'][index] = 0\n",
            "<ipython-input-7-cee5a35a340a>:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['anger'][index] = 0\n",
            "<ipython-input-7-cee5a35a340a>:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['suspense'][index] = 0\n",
            "<ipython-input-7-cee5a35a340a>:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['sad'][index] = 1\n",
            "<ipython-input-7-cee5a35a340a>:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['neutral'][index] = 0\n",
            "<ipython-input-7-cee5a35a340a>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['joy'][index] = 0\n",
            "<ipython-input-7-cee5a35a340a>:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['anger'][index] = 0\n",
            "<ipython-input-7-cee5a35a340a>:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['suspense'][index] = 1\n",
            "<ipython-input-7-cee5a35a340a>:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['sad'][index] = 0\n",
            "<ipython-input-7-cee5a35a340a>:26: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df['neutral'][index] = 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Sentences  Annotation  joy  \\\n",
              "0               रमजान के पूरे तीस रोजों के बाद ईद आयी है           1    1   \n",
              "1                   कितना मनोहर, कितना सुहावना प्रभाव है           1    1   \n",
              "2      वृक्षों पर अजीब हरियाली है, खेतों में कुछ अजीब...           1    1   \n",
              "3      आज का सूर्य देखो, कितना प्यारा, कितना शीतल है,...           1    1   \n",
              "4                                 गाँव में कितनी हलचल है           1    1   \n",
              "...                                                  ...         ...  ...   \n",
              "20299                          फिर यहाँ सर्दी हो जाती है           4    0   \n",
              "20300  दिन-भर मैं यह देखती रहती हूँ कि धूप का टुकड़ा क...           4    0   \n",
              "20301  पार्क का कोई ऐसा कोना नहीं, जहाँ मैं घड़ी-आधा घ...           4    0   \n",
              "20302              लेकिन यह बेंच मुझे सबसे अच्छी लगती है           1    1   \n",
              "20303  एक तो इस पर पत्ते नहीं झरते और दूसरे... अरे, आ...           4    0   \n",
              "\n",
              "       anger  suspense  sad  neutral  \n",
              "0          0         0    0        0  \n",
              "1          0         0    0        0  \n",
              "2          0         0    0        0  \n",
              "3          0         0    0        0  \n",
              "4          0         0    0        0  \n",
              "...      ...       ...  ...      ...  \n",
              "20299      0         0    0        1  \n",
              "20300      0         0    0        1  \n",
              "20301      0         0    0        1  \n",
              "20302      0         0    0        0  \n",
              "20303      0         0    0        1  \n",
              "\n",
              "[20304 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3e3ac0f3-77e9-401c-b6be-adef94a4e420\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentences</th>\n",
              "      <th>Annotation</th>\n",
              "      <th>joy</th>\n",
              "      <th>anger</th>\n",
              "      <th>suspense</th>\n",
              "      <th>sad</th>\n",
              "      <th>neutral</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>रमजान के पूरे तीस रोजों के बाद ईद आयी है</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>कितना मनोहर, कितना सुहावना प्रभाव है</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>वृक्षों पर अजीब हरियाली है, खेतों में कुछ अजीब...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>आज का सूर्य देखो, कितना प्यारा, कितना शीतल है,...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>गाँव में कितनी हलचल है</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20299</th>\n",
              "      <td>फिर यहाँ सर्दी हो जाती है</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20300</th>\n",
              "      <td>दिन-भर मैं यह देखती रहती हूँ कि धूप का टुकड़ा क...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20301</th>\n",
              "      <td>पार्क का कोई ऐसा कोना नहीं, जहाँ मैं घड़ी-आधा घ...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20302</th>\n",
              "      <td>लेकिन यह बेंच मुझे सबसे अच्छी लगती है</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20303</th>\n",
              "      <td>एक तो इस पर पत्ते नहीं झरते और दूसरे... अरे, आ...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20304 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3e3ac0f3-77e9-401c-b6be-adef94a4e420')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3e3ac0f3-77e9-401c-b6be-adef94a4e420 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3e3ac0f3-77e9-401c-b6be-adef94a4e420');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categories = train_df.columns[2:]\n",
        "counts = []\n",
        "for category in categories:\n",
        "    counts.append((category, train_df[category].sum()))\n",
        "df_stats = pd.DataFrame(counts, columns=['category', 'number of comments'])\n",
        "df_stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ImXzHi3fEs1A",
        "outputId": "b0d3608a-cfa4-47c0-ad90-da1ac207d750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   category  number of comments\n",
              "0       joy                2463\n",
              "1     anger                1464\n",
              "2  suspense                1512\n",
              "3       sad                3168\n",
              "4   neutral               11697"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d329b239-7379-4ce9-8199-5109c20018e3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>number of comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>joy</td>\n",
              "      <td>2463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>anger</td>\n",
              "      <td>1464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>suspense</td>\n",
              "      <td>1512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sad</td>\n",
              "      <td>3168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>neutral</td>\n",
              "      <td>11697</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d329b239-7379-4ce9-8199-5109c20018e3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d329b239-7379-4ce9-8199-5109c20018e3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d329b239-7379-4ce9-8199-5109c20018e3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_list = categories"
      ],
      "metadata": {
        "id": "Ma2luVKBE1ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizeWithBert(example):\n",
        "  encodings = tokenizer.encode_plus(\n",
        "    example,\n",
        "    add_special_tokens = True,   # tokens CLS, PAD, SEP\n",
        "    max_length = 512, #MAX_LEN\n",
        "    padding = 'max_length',\n",
        "    truncation = True,\n",
        "    return_attention_mask = True,\n",
        "    return_tensors = 'pt'\n",
        "  )\n",
        "  return encodings"
      ],
      "metadata": {
        "id": "efJexpczE5DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(df, tokenizer, mode='train'):\n",
        "    sentences, labels = df['Sentences'], df.iloc[:,2:].to_numpy()\n",
        "    max_length = 300\n",
        "    in_T = []\n",
        "    in_T_attn_masks = []\n",
        "    for sentence in sentences:\n",
        "        enc_sent_dict = tokenizer.encode_plus(\n",
        "            sentence[:300],\n",
        "            max_length = max_length,\n",
        "            add_special_tokens = True,\n",
        "            pad_to_max_length = True,\n",
        "            return_attention_mask = True,\n",
        "            return_tensors = 'pt'\n",
        "        )\n",
        "        in_T.append(enc_sent_dict['input_ids'])\n",
        "        in_T_attn_masks.append(enc_sent_dict['attention_mask'])\n",
        "    \n",
        "    in_T = torch.cat(in_T, dim=0)\n",
        "    in_T_attn_masks = torch.cat(in_T_attn_masks, dim=0)\n",
        "    labels = torch.tensor(labels, dtype = torch.float32)\n",
        "    print('Text Input: ' , in_T.shape)\n",
        "    print('Text Input Attention: ' , in_T_attn_masks.shape)    \n",
        "    print('Labels: ' , labels.shape)\n",
        "    \n",
        "    dataset = TensorDataset(\n",
        "        in_T,\n",
        "        in_T_attn_masks,\n",
        "        labels\n",
        "    )\n",
        "    \n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    \n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    return train_dataset, val_dataset"
      ],
      "metadata": {
        "id": "E-GxCc8dE849"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "train_dataset, val_dataset = get_dataset(\n",
        "    train_df,\n",
        "    tokenizer = tokenizer,\n",
        "    mode = 'train'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "adaffcea97324b589bd445a1bbab3f10",
            "a44b048586b94eb6a76169235b74782a",
            "ce1172eafbb6441e8ec1d9a221d86178",
            "030a4c83d60e4dd6b3ad8ef7d747544c",
            "f2edddf29f36441b86a08ee10b968929",
            "ce857b0176674e3cbc750a723fe4129f",
            "4fc0ee947a474cc6ae76c0732ebb0572",
            "d8f1e34436154ce8b0e852d17d07e088",
            "d1dd21de049e4f65920bc58e98841549",
            "4248add1db994b71bdcfbb196a9355e5",
            "d61cd0792fb94c9092900248ae7475ae",
            "16d7c92d9a7d414ead73af526cf60746",
            "392898fe88b8404b8736d74576b98081",
            "d17fae9256d84331b8dcadca11935dbf",
            "784da13a9524475d9b99860026b51451",
            "39fa392b43714f49a8eff3855874af02",
            "9a5cce6cb8664a1582cc4db9f612a048",
            "ac91e565b99348769a66bb67f1b88b6c",
            "bf6dbea5f41d466094ee2d517c6f7552",
            "ce8933f1ea244ad2b159982ce08efaae",
            "5a9666140a79455abeec266ac3e3993f",
            "32baab1a11154ce69922d74ce885bfbf",
            "c37b7109c8284b0595f6b83717336dbf",
            "e9616c48a1124bfead8b02cd181a0030",
            "b14c8cc7a2c94d15ac2688ea7717e578",
            "8075b0fb90364788a8d1234b682f7fd7",
            "bff11d95501d4911b7642ddf2bf74313",
            "0f8821b436f3418b95f79a63fc1cb78d",
            "bd202bc07e3d43988eb55bffca0a675c",
            "08527858e6a4429ab8e60b64de7fb0e5",
            "d66222dee4944df3a2c0a3181afec35b",
            "9183b0e712ce4062af3483599ec376b9",
            "a6e827a696a94576b25e3360761a231d"
          ]
        },
        "id": "__CrNizjFAtT",
        "outputId": "ea95a108-6b4c-4244-8683-cdbff7f7cdee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "adaffcea97324b589bd445a1bbab3f10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16d7c92d9a7d414ead73af526cf60746"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c37b7109c8284b0595f6b83717336dbf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Input:  torch.Size([20304, 300])\n",
            "Text Input Attention:  torch.Size([20304, 300])\n",
            "Labels:  torch.Size([20304, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    sampler = RandomSampler(train_dataset)\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    sampler = SequentialSampler(val_dataset)\n",
        ")\n",
        "\n",
        "print('Data Ready!!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTGJQWP5FDq-",
        "outputId": "c545fd42-fe1e-4926-b094-8ecc35b0eedc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Ready!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from transformers import BertModel\n",
        "\n",
        "class MultiClassClassifier(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_labels):\n",
        "        super(MultiClassClassifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_labels = num_labels\n",
        "        \n",
        "        self.bertmodel = BertModel.from_pretrained('ai4bharat/indic-bert')\n",
        "        self.ffn1 = nn.Linear(768, hidden_dim)\n",
        "        self.dp1 = nn.Dropout()\n",
        "        self.ffn2 = nn.Linear(hidden_dim, num_labels)\n",
        "        \n",
        "    def forward(self, in_T, in_T_attn_masks):\n",
        "        outputs = self.bertmodel(in_T, in_T_attn_masks)\n",
        "        #from 3.x onwards use outputs.last_hidden_state\n",
        "        x = torch.mean(outputs.last_hidden_state, dim=1) #from 3.x onwards use outputs.last_hidden_state\n",
        "        x = F.relu(self.ffn1(x))\n",
        "        x = self.dp1(x)\n",
        "        x = torch.sigmoid(self.ffn2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "En0OvVjTFKZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#format time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "6C1Qvb_PFQU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiClassClassifier(100, 5).to(device) # 100 hidden dimension, 2 lables\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8) # Adam with weight decay\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "8e6e73b934ec41d196afc76572ddfdca",
            "450bae7f3406478595f13d3f140bc1d5",
            "5476ac654fc64834a93e630ae25f1dab",
            "c0e26ab765e64d52b8ac87d39498a4a4",
            "1e9ff4e452234deb829353c20746ddfd",
            "10ced279f44f47d6bb87f5c462c0ea0c",
            "18ca6ae27d1445d994ab459e9734953a",
            "c8ec69ad64954bea93bcf1a2af04dbcf",
            "6a589863a436454a8bc4b258dda4d024",
            "90ad78be101b4799895c93a4d4f45054",
            "b1733b2753414e6b8dd881948f9045cb",
            "1b6f61623dbf40fcaae63c017a79b140",
            "48f3e38c0cab4385ac14649412cbddea",
            "5b26cf5b8b804738ad7a4717dcd02ff6",
            "e95bccef365b465fbb28736b060fc3f9",
            "bae4de9d7c5547d7ac8ddc76c1031637",
            "3812630c3c5c46abbb55a441b9494cdc",
            "7d6f447b8fb2463c8dcbdd5884e93486",
            "8a3b695e7e404284a97406872187181a",
            "fa85df07d1774423b77a8e36b37f4408",
            "2acb6900c9984c0c95ea7e2cbb83a196",
            "e60999a69c6c42b9a83c1047395741c6"
          ]
        },
        "id": "yie0ORFmFTOW",
        "outputId": "cdb26b6e-1a3b-455d-c080-513a37c2cc6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e6e73b934ec41d196afc76572ddfdca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type albert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b6f61623dbf40fcaae63c017a79b140"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ai4bharat/indic-bert were not used when initializing BertModel: ['albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias', 'predictions.dense.bias', 'albert.pooler.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight', 'albert.embeddings.token_type_embeddings.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight', 'albert.encoder.embedding_hidden_mapping_in.weight', 'sop_classifier.classifier.bias', 'albert.embeddings.position_embeddings.weight', 'predictions.bias', 'sop_classifier.classifier.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias', 'predictions.dense.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight', 'predictions.decoder.weight', 'albert.embeddings.LayerNorm.bias', 'albert.encoder.embedding_hidden_mapping_in.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias', 'albert.embeddings.LayerNorm.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight', 'albert.embeddings.word_embeddings.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias', 'predictions.LayerNorm.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'albert.pooler.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['encoder.layer.8.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.self.query.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAINING and VALIDATION\n",
        "epochs = 3   #5, reduced to one epoch as it is taking lot of time\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                           num_warmup_steps = 0,\n",
        "                                           num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "\n",
        "best_val_loss = 1e8\n",
        "true_labels = val_dataset[:][2].numpy()\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    #############               Training\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        if step % 5 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}. Loss: {:.5f}'.format(step, len(train_dataloader), elapsed, total_train_loss/step))\n",
        "\n",
        "        b_in_T            = batch[0].to(device)\n",
        "        b_in_T_attn_masks = batch[1].to(device)\n",
        "        b_labels          = batch[2].to(device)\n",
        "        \n",
        "        model.zero_grad()\n",
        "\n",
        "        logits = model(b_in_T, b_in_T_attn_masks)\n",
        "        loss = criterion(logits, b_labels)\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "    ##########               Validation\n",
        "   \n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    pred_labels = np.empty((0,5))\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in val_dataloader:\n",
        "        \n",
        "        b_in_T            = batch[0].to(device)\n",
        "        b_in_T_attn_masks = batch[1].to(device)\n",
        "        b_labels          = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_in_T, b_in_T_attn_masks)\n",
        "            loss = criterion(logits, b_labels)\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        pred_labels = np.concatenate((pred_labels, logits), axis=0)\n",
        "\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(val_dataloader)\n",
        "\n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    pred_labels = np.array([[int(x >= 0.25) for x in pred_labels[:,i]] for i  in range(5)]).transpose()\n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "#     Report the final accuracy, f1-score for this validation run.\n",
        "    for i in range(5):\n",
        "        print(\"  Accuracy: {0:.2f}\".format(accuracy_score(true_labels[:,i], pred_labels[:,i])))\n",
        "\n",
        "    for i in range(5):\n",
        "        print(\"  Macro F1-score: {0:.2f}\".format(f1_score(true_labels[:,i], pred_labels[:,i], average='macro')))\n",
        "\n",
        "    for i in range(5):\n",
        "        print(\"  Weighted F1-score: {0:.2f}\".format(f1_score(true_labels[:,i], pred_labels[:,i], average='weighted')))\n",
        "\n",
        "    print('Classification Report:')\n",
        "    for i in range(5):\n",
        "        print(classification_report(true_labels[:,i], pred_labels[:,i]))\n",
        "\n",
        "    print('Confusion Matrix:')\n",
        "    for i in range(2):\n",
        "        print(confusion_matrix(true_labels[:,i], pred_labels[:,i]))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'training_loss': avg_train_loss,\n",
        "            'val_loss': avg_val_loss,\n",
        "            'val_accuracy': np.mean([accuracy_score(true_labels[:,i], pred_labels[:,i]) for i in range(5)]),\n",
        "            'val_macro_f1': np.mean([f1_score(true_labels[:,i], pred_labels[:,i], average='macro') for i in range(5)]),\n",
        "            'val_weighted_f1': np.mean([f1_score(true_labels[:,i], pred_labels[:,i], average='weighted') for i in range(5)]),\n",
        "            'training_time': training_time,\n",
        "            'val_tim': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "    model_path = 'model_state_dict_'+str(epoch_i)+'.pt'\n",
        "    torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0u-EPjdCFVK8",
        "outputId": "004262f1-93e7-4d50-d4c9-8a7ee444a7ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch     5  of  2,031.    Elapsed: 0:00:06. Loss: 0.49849\n",
            "  Batch    10  of  2,031.    Elapsed: 0:00:08. Loss: 0.51742\n",
            "  Batch    15  of  2,031.    Elapsed: 0:00:10. Loss: 0.50135\n",
            "  Batch    20  of  2,031.    Elapsed: 0:00:13. Loss: 0.47746\n",
            "  Batch    25  of  2,031.    Elapsed: 0:00:15. Loss: 0.47367\n",
            "  Batch    30  of  2,031.    Elapsed: 0:00:17. Loss: 0.48007\n",
            "  Batch    35  of  2,031.    Elapsed: 0:00:19. Loss: 0.47084\n",
            "  Batch    40  of  2,031.    Elapsed: 0:00:21. Loss: 0.46624\n",
            "  Batch    45  of  2,031.    Elapsed: 0:00:24. Loss: 0.46815\n",
            "  Batch    50  of  2,031.    Elapsed: 0:00:26. Loss: 0.46270\n",
            "  Batch    55  of  2,031.    Elapsed: 0:00:28. Loss: 0.45859\n",
            "  Batch    60  of  2,031.    Elapsed: 0:00:30. Loss: 0.45427\n",
            "  Batch    65  of  2,031.    Elapsed: 0:00:33. Loss: 0.46294\n",
            "  Batch    70  of  2,031.    Elapsed: 0:00:35. Loss: 0.46320\n",
            "  Batch    75  of  2,031.    Elapsed: 0:00:37. Loss: 0.46480\n",
            "  Batch    80  of  2,031.    Elapsed: 0:00:39. Loss: 0.46030\n",
            "  Batch    85  of  2,031.    Elapsed: 0:00:42. Loss: 0.45923\n",
            "  Batch    90  of  2,031.    Elapsed: 0:00:44. Loss: 0.46041\n",
            "  Batch    95  of  2,031.    Elapsed: 0:00:46. Loss: 0.45436\n",
            "  Batch   100  of  2,031.    Elapsed: 0:00:49. Loss: 0.45291\n",
            "  Batch   105  of  2,031.    Elapsed: 0:00:51. Loss: 0.45101\n",
            "  Batch   110  of  2,031.    Elapsed: 0:00:53. Loss: 0.44919\n",
            "  Batch   115  of  2,031.    Elapsed: 0:00:55. Loss: 0.44963\n",
            "  Batch   120  of  2,031.    Elapsed: 0:00:58. Loss: 0.44883\n",
            "  Batch   125  of  2,031.    Elapsed: 0:01:00. Loss: 0.44908\n",
            "  Batch   130  of  2,031.    Elapsed: 0:01:02. Loss: 0.44728\n",
            "  Batch   135  of  2,031.    Elapsed: 0:01:05. Loss: 0.44696\n",
            "  Batch   140  of  2,031.    Elapsed: 0:01:07. Loss: 0.44741\n",
            "  Batch   145  of  2,031.    Elapsed: 0:01:09. Loss: 0.44554\n",
            "  Batch   150  of  2,031.    Elapsed: 0:01:11. Loss: 0.44865\n",
            "  Batch   155  of  2,031.    Elapsed: 0:01:14. Loss: 0.44729\n",
            "  Batch   160  of  2,031.    Elapsed: 0:01:16. Loss: 0.44774\n",
            "  Batch   165  of  2,031.    Elapsed: 0:01:18. Loss: 0.44580\n",
            "  Batch   170  of  2,031.    Elapsed: 0:01:21. Loss: 0.44441\n",
            "  Batch   175  of  2,031.    Elapsed: 0:01:23. Loss: 0.44324\n",
            "  Batch   180  of  2,031.    Elapsed: 0:01:25. Loss: 0.44566\n",
            "  Batch   185  of  2,031.    Elapsed: 0:01:28. Loss: 0.44671\n",
            "  Batch   190  of  2,031.    Elapsed: 0:01:30. Loss: 0.44728\n",
            "  Batch   195  of  2,031.    Elapsed: 0:01:32. Loss: 0.44653\n",
            "  Batch   200  of  2,031.    Elapsed: 0:01:35. Loss: 0.44626\n",
            "  Batch   205  of  2,031.    Elapsed: 0:01:37. Loss: 0.44711\n",
            "  Batch   210  of  2,031.    Elapsed: 0:01:40. Loss: 0.44605\n",
            "  Batch   215  of  2,031.    Elapsed: 0:01:42. Loss: 0.44527\n",
            "  Batch   220  of  2,031.    Elapsed: 0:01:44. Loss: 0.44422\n",
            "  Batch   225  of  2,031.    Elapsed: 0:01:47. Loss: 0.44453\n",
            "  Batch   230  of  2,031.    Elapsed: 0:01:49. Loss: 0.44426\n",
            "  Batch   235  of  2,031.    Elapsed: 0:01:52. Loss: 0.44318\n",
            "  Batch   240  of  2,031.    Elapsed: 0:01:54. Loss: 0.44213\n",
            "  Batch   245  of  2,031.    Elapsed: 0:01:56. Loss: 0.44214\n",
            "  Batch   250  of  2,031.    Elapsed: 0:01:59. Loss: 0.44080\n",
            "  Batch   255  of  2,031.    Elapsed: 0:02:01. Loss: 0.44110\n",
            "  Batch   260  of  2,031.    Elapsed: 0:02:04. Loss: 0.43932\n",
            "  Batch   265  of  2,031.    Elapsed: 0:02:06. Loss: 0.43827\n",
            "  Batch   270  of  2,031.    Elapsed: 0:02:08. Loss: 0.43838\n",
            "  Batch   275  of  2,031.    Elapsed: 0:02:11. Loss: 0.43989\n",
            "  Batch   280  of  2,031.    Elapsed: 0:02:13. Loss: 0.43930\n",
            "  Batch   285  of  2,031.    Elapsed: 0:02:16. Loss: 0.43990\n",
            "  Batch   290  of  2,031.    Elapsed: 0:02:18. Loss: 0.43980\n",
            "  Batch   295  of  2,031.    Elapsed: 0:02:20. Loss: 0.43885\n",
            "  Batch   300  of  2,031.    Elapsed: 0:02:23. Loss: 0.43926\n",
            "  Batch   305  of  2,031.    Elapsed: 0:02:25. Loss: 0.43918\n",
            "  Batch   310  of  2,031.    Elapsed: 0:02:27. Loss: 0.43921\n",
            "  Batch   315  of  2,031.    Elapsed: 0:02:30. Loss: 0.43776\n",
            "  Batch   320  of  2,031.    Elapsed: 0:02:32. Loss: 0.43767\n",
            "  Batch   325  of  2,031.    Elapsed: 0:02:35. Loss: 0.43645\n",
            "  Batch   330  of  2,031.    Elapsed: 0:02:37. Loss: 0.43622\n",
            "  Batch   335  of  2,031.    Elapsed: 0:02:39. Loss: 0.43511\n",
            "  Batch   340  of  2,031.    Elapsed: 0:02:42. Loss: 0.43615\n",
            "  Batch   345  of  2,031.    Elapsed: 0:02:44. Loss: 0.43635\n",
            "  Batch   350  of  2,031.    Elapsed: 0:02:46. Loss: 0.43658\n",
            "  Batch   355  of  2,031.    Elapsed: 0:02:49. Loss: 0.43616\n",
            "  Batch   360  of  2,031.    Elapsed: 0:02:51. Loss: 0.43635\n",
            "  Batch   365  of  2,031.    Elapsed: 0:02:53. Loss: 0.43659\n",
            "  Batch   370  of  2,031.    Elapsed: 0:02:56. Loss: 0.43577\n",
            "  Batch   375  of  2,031.    Elapsed: 0:02:58. Loss: 0.43515\n",
            "  Batch   380  of  2,031.    Elapsed: 0:03:01. Loss: 0.43547\n",
            "  Batch   385  of  2,031.    Elapsed: 0:03:03. Loss: 0.43568\n",
            "  Batch   390  of  2,031.    Elapsed: 0:03:05. Loss: 0.43540\n",
            "  Batch   395  of  2,031.    Elapsed: 0:03:08. Loss: 0.43576\n",
            "  Batch   400  of  2,031.    Elapsed: 0:03:10. Loss: 0.43551\n",
            "  Batch   405  of  2,031.    Elapsed: 0:03:12. Loss: 0.43517\n",
            "  Batch   410  of  2,031.    Elapsed: 0:03:15. Loss: 0.43641\n",
            "  Batch   415  of  2,031.    Elapsed: 0:03:17. Loss: 0.43700\n",
            "  Batch   420  of  2,031.    Elapsed: 0:03:20. Loss: 0.43747\n",
            "  Batch   425  of  2,031.    Elapsed: 0:03:22. Loss: 0.43718\n",
            "  Batch   430  of  2,031.    Elapsed: 0:03:24. Loss: 0.43696\n",
            "  Batch   435  of  2,031.    Elapsed: 0:03:27. Loss: 0.43709\n",
            "  Batch   440  of  2,031.    Elapsed: 0:03:29. Loss: 0.43609\n",
            "  Batch   445  of  2,031.    Elapsed: 0:03:32. Loss: 0.43532\n",
            "  Batch   450  of  2,031.    Elapsed: 0:03:34. Loss: 0.43603\n",
            "  Batch   455  of  2,031.    Elapsed: 0:03:36. Loss: 0.43588\n",
            "  Batch   460  of  2,031.    Elapsed: 0:03:39. Loss: 0.43591\n",
            "  Batch   465  of  2,031.    Elapsed: 0:03:41. Loss: 0.43524\n",
            "  Batch   470  of  2,031.    Elapsed: 0:03:43. Loss: 0.43513\n",
            "  Batch   475  of  2,031.    Elapsed: 0:03:46. Loss: 0.43532\n",
            "  Batch   480  of  2,031.    Elapsed: 0:03:48. Loss: 0.43494\n",
            "  Batch   485  of  2,031.    Elapsed: 0:03:51. Loss: 0.43515\n",
            "  Batch   490  of  2,031.    Elapsed: 0:03:53. Loss: 0.43499\n",
            "  Batch   495  of  2,031.    Elapsed: 0:03:55. Loss: 0.43531\n",
            "  Batch   500  of  2,031.    Elapsed: 0:03:58. Loss: 0.43456\n",
            "  Batch   505  of  2,031.    Elapsed: 0:04:00. Loss: 0.43520\n",
            "  Batch   510  of  2,031.    Elapsed: 0:04:03. Loss: 0.43464\n",
            "  Batch   515  of  2,031.    Elapsed: 0:04:05. Loss: 0.43416\n",
            "  Batch   520  of  2,031.    Elapsed: 0:04:07. Loss: 0.43425\n",
            "  Batch   525  of  2,031.    Elapsed: 0:04:10. Loss: 0.43461\n",
            "  Batch   530  of  2,031.    Elapsed: 0:04:12. Loss: 0.43525\n",
            "  Batch   535  of  2,031.    Elapsed: 0:04:14. Loss: 0.43516\n",
            "  Batch   540  of  2,031.    Elapsed: 0:04:17. Loss: 0.43452\n",
            "  Batch   545  of  2,031.    Elapsed: 0:04:19. Loss: 0.43389\n",
            "  Batch   550  of  2,031.    Elapsed: 0:04:22. Loss: 0.43365\n",
            "  Batch   555  of  2,031.    Elapsed: 0:04:24. Loss: 0.43359\n",
            "  Batch   560  of  2,031.    Elapsed: 0:04:26. Loss: 0.43339\n",
            "  Batch   565  of  2,031.    Elapsed: 0:04:29. Loss: 0.43323\n",
            "  Batch   570  of  2,031.    Elapsed: 0:04:31. Loss: 0.43291\n",
            "  Batch   575  of  2,031.    Elapsed: 0:04:33. Loss: 0.43331\n",
            "  Batch   580  of  2,031.    Elapsed: 0:04:36. Loss: 0.43292\n",
            "  Batch   585  of  2,031.    Elapsed: 0:04:38. Loss: 0.43319\n",
            "  Batch   590  of  2,031.    Elapsed: 0:04:41. Loss: 0.43350\n",
            "  Batch   595  of  2,031.    Elapsed: 0:04:43. Loss: 0.43401\n",
            "  Batch   600  of  2,031.    Elapsed: 0:04:45. Loss: 0.43367\n",
            "  Batch   605  of  2,031.    Elapsed: 0:04:48. Loss: 0.43375\n",
            "  Batch   610  of  2,031.    Elapsed: 0:04:50. Loss: 0.43365\n",
            "  Batch   615  of  2,031.    Elapsed: 0:04:52. Loss: 0.43403\n",
            "  Batch   620  of  2,031.    Elapsed: 0:04:55. Loss: 0.43386\n",
            "  Batch   625  of  2,031.    Elapsed: 0:04:57. Loss: 0.43360\n",
            "  Batch   630  of  2,031.    Elapsed: 0:05:00. Loss: 0.43346\n",
            "  Batch   635  of  2,031.    Elapsed: 0:05:02. Loss: 0.43360\n",
            "  Batch   640  of  2,031.    Elapsed: 0:05:04. Loss: 0.43299\n",
            "  Batch   645  of  2,031.    Elapsed: 0:05:07. Loss: 0.43339\n",
            "  Batch   650  of  2,031.    Elapsed: 0:05:09. Loss: 0.43353\n",
            "  Batch   655  of  2,031.    Elapsed: 0:05:11. Loss: 0.43335\n",
            "  Batch   660  of  2,031.    Elapsed: 0:05:14. Loss: 0.43348\n",
            "  Batch   665  of  2,031.    Elapsed: 0:05:16. Loss: 0.43378\n",
            "  Batch   670  of  2,031.    Elapsed: 0:05:19. Loss: 0.43422\n",
            "  Batch   675  of  2,031.    Elapsed: 0:05:21. Loss: 0.43412\n",
            "  Batch   680  of  2,031.    Elapsed: 0:05:23. Loss: 0.43410\n",
            "  Batch   685  of  2,031.    Elapsed: 0:05:26. Loss: 0.43401\n",
            "  Batch   690  of  2,031.    Elapsed: 0:05:28. Loss: 0.43351\n",
            "  Batch   695  of  2,031.    Elapsed: 0:05:31. Loss: 0.43360\n",
            "  Batch   700  of  2,031.    Elapsed: 0:05:33. Loss: 0.43355\n",
            "  Batch   705  of  2,031.    Elapsed: 0:05:35. Loss: 0.43363\n",
            "  Batch   710  of  2,031.    Elapsed: 0:05:38. Loss: 0.43362\n",
            "  Batch   715  of  2,031.    Elapsed: 0:05:40. Loss: 0.43407\n",
            "  Batch   720  of  2,031.    Elapsed: 0:05:42. Loss: 0.43373\n",
            "  Batch   725  of  2,031.    Elapsed: 0:05:45. Loss: 0.43373\n",
            "  Batch   730  of  2,031.    Elapsed: 0:05:47. Loss: 0.43376\n",
            "  Batch   735  of  2,031.    Elapsed: 0:05:50. Loss: 0.43345\n",
            "  Batch   740  of  2,031.    Elapsed: 0:05:52. Loss: 0.43366\n",
            "  Batch   745  of  2,031.    Elapsed: 0:05:54. Loss: 0.43365\n",
            "  Batch   750  of  2,031.    Elapsed: 0:05:57. Loss: 0.43318\n",
            "  Batch   755  of  2,031.    Elapsed: 0:05:59. Loss: 0.43335\n",
            "  Batch   760  of  2,031.    Elapsed: 0:06:01. Loss: 0.43328\n",
            "  Batch   765  of  2,031.    Elapsed: 0:06:04. Loss: 0.43327\n",
            "  Batch   770  of  2,031.    Elapsed: 0:06:06. Loss: 0.43363\n",
            "  Batch   775  of  2,031.    Elapsed: 0:06:09. Loss: 0.43355\n",
            "  Batch   780  of  2,031.    Elapsed: 0:06:11. Loss: 0.43345\n",
            "  Batch   785  of  2,031.    Elapsed: 0:06:13. Loss: 0.43361\n",
            "  Batch   790  of  2,031.    Elapsed: 0:06:16. Loss: 0.43350\n",
            "  Batch   795  of  2,031.    Elapsed: 0:06:18. Loss: 0.43304\n",
            "  Batch   800  of  2,031.    Elapsed: 0:06:20. Loss: 0.43276\n",
            "  Batch   805  of  2,031.    Elapsed: 0:06:23. Loss: 0.43312\n",
            "  Batch   810  of  2,031.    Elapsed: 0:06:25. Loss: 0.43306\n",
            "  Batch   815  of  2,031.    Elapsed: 0:06:28. Loss: 0.43331\n",
            "  Batch   820  of  2,031.    Elapsed: 0:06:30. Loss: 0.43327\n",
            "  Batch   825  of  2,031.    Elapsed: 0:06:32. Loss: 0.43348\n",
            "  Batch   830  of  2,031.    Elapsed: 0:06:35. Loss: 0.43364\n",
            "  Batch   835  of  2,031.    Elapsed: 0:06:37. Loss: 0.43313\n",
            "  Batch   840  of  2,031.    Elapsed: 0:06:40. Loss: 0.43299\n",
            "  Batch   845  of  2,031.    Elapsed: 0:06:42. Loss: 0.43262\n",
            "  Batch   850  of  2,031.    Elapsed: 0:06:44. Loss: 0.43292\n",
            "  Batch   855  of  2,031.    Elapsed: 0:06:47. Loss: 0.43274\n",
            "  Batch   860  of  2,031.    Elapsed: 0:06:49. Loss: 0.43300\n",
            "  Batch   865  of  2,031.    Elapsed: 0:06:51. Loss: 0.43307\n",
            "  Batch   870  of  2,031.    Elapsed: 0:06:54. Loss: 0.43289\n",
            "  Batch   875  of  2,031.    Elapsed: 0:06:56. Loss: 0.43293\n",
            "  Batch   880  of  2,031.    Elapsed: 0:06:59. Loss: 0.43271\n",
            "  Batch   885  of  2,031.    Elapsed: 0:07:01. Loss: 0.43277\n",
            "  Batch   890  of  2,031.    Elapsed: 0:07:03. Loss: 0.43259\n",
            "  Batch   895  of  2,031.    Elapsed: 0:07:06. Loss: 0.43283\n",
            "  Batch   900  of  2,031.    Elapsed: 0:07:08. Loss: 0.43301\n",
            "  Batch   905  of  2,031.    Elapsed: 0:07:10. Loss: 0.43301\n",
            "  Batch   910  of  2,031.    Elapsed: 0:07:13. Loss: 0.43290\n",
            "  Batch   915  of  2,031.    Elapsed: 0:07:15. Loss: 0.43312\n",
            "  Batch   920  of  2,031.    Elapsed: 0:07:18. Loss: 0.43292\n",
            "  Batch   925  of  2,031.    Elapsed: 0:07:20. Loss: 0.43296\n",
            "  Batch   930  of  2,031.    Elapsed: 0:07:22. Loss: 0.43256\n",
            "  Batch   935  of  2,031.    Elapsed: 0:07:25. Loss: 0.43269\n",
            "  Batch   940  of  2,031.    Elapsed: 0:07:27. Loss: 0.43238\n",
            "  Batch   945  of  2,031.    Elapsed: 0:07:29. Loss: 0.43236\n",
            "  Batch   950  of  2,031.    Elapsed: 0:07:32. Loss: 0.43223\n",
            "  Batch   955  of  2,031.    Elapsed: 0:07:34. Loss: 0.43223\n",
            "  Batch   960  of  2,031.    Elapsed: 0:07:37. Loss: 0.43216\n",
            "  Batch   965  of  2,031.    Elapsed: 0:07:39. Loss: 0.43180\n",
            "  Batch   970  of  2,031.    Elapsed: 0:07:41. Loss: 0.43123\n",
            "  Batch   975  of  2,031.    Elapsed: 0:07:44. Loss: 0.43162\n",
            "  Batch   980  of  2,031.    Elapsed: 0:07:46. Loss: 0.43176\n",
            "  Batch   985  of  2,031.    Elapsed: 0:07:48. Loss: 0.43205\n",
            "  Batch   990  of  2,031.    Elapsed: 0:07:51. Loss: 0.43219\n",
            "  Batch   995  of  2,031.    Elapsed: 0:07:53. Loss: 0.43214\n",
            "  Batch 1,000  of  2,031.    Elapsed: 0:07:56. Loss: 0.43193\n",
            "  Batch 1,005  of  2,031.    Elapsed: 0:07:58. Loss: 0.43180\n",
            "  Batch 1,010  of  2,031.    Elapsed: 0:08:00. Loss: 0.43147\n",
            "  Batch 1,015  of  2,031.    Elapsed: 0:08:03. Loss: 0.43125\n",
            "  Batch 1,020  of  2,031.    Elapsed: 0:08:05. Loss: 0.43118\n",
            "  Batch 1,025  of  2,031.    Elapsed: 0:08:08. Loss: 0.43109\n",
            "  Batch 1,030  of  2,031.    Elapsed: 0:08:10. Loss: 0.43110\n",
            "  Batch 1,035  of  2,031.    Elapsed: 0:08:12. Loss: 0.43121\n",
            "  Batch 1,040  of  2,031.    Elapsed: 0:08:15. Loss: 0.43089\n",
            "  Batch 1,045  of  2,031.    Elapsed: 0:08:17. Loss: 0.43107\n",
            "  Batch 1,050  of  2,031.    Elapsed: 0:08:19. Loss: 0.43112\n",
            "  Batch 1,055  of  2,031.    Elapsed: 0:08:22. Loss: 0.43106\n",
            "  Batch 1,060  of  2,031.    Elapsed: 0:08:24. Loss: 0.43096\n",
            "  Batch 1,065  of  2,031.    Elapsed: 0:08:27. Loss: 0.43099\n",
            "  Batch 1,070  of  2,031.    Elapsed: 0:08:29. Loss: 0.43062\n",
            "  Batch 1,075  of  2,031.    Elapsed: 0:08:31. Loss: 0.43032\n",
            "  Batch 1,080  of  2,031.    Elapsed: 0:08:34. Loss: 0.42993\n",
            "  Batch 1,085  of  2,031.    Elapsed: 0:08:36. Loss: 0.43009\n",
            "  Batch 1,090  of  2,031.    Elapsed: 0:08:38. Loss: 0.42994\n",
            "  Batch 1,095  of  2,031.    Elapsed: 0:08:41. Loss: 0.42988\n",
            "  Batch 1,100  of  2,031.    Elapsed: 0:08:43. Loss: 0.42998\n",
            "  Batch 1,105  of  2,031.    Elapsed: 0:08:46. Loss: 0.43025\n",
            "  Batch 1,110  of  2,031.    Elapsed: 0:08:48. Loss: 0.43019\n",
            "  Batch 1,115  of  2,031.    Elapsed: 0:08:50. Loss: 0.43036\n",
            "  Batch 1,120  of  2,031.    Elapsed: 0:08:53. Loss: 0.43036\n",
            "  Batch 1,125  of  2,031.    Elapsed: 0:08:55. Loss: 0.43025\n",
            "  Batch 1,130  of  2,031.    Elapsed: 0:08:58. Loss: 0.43029\n",
            "  Batch 1,135  of  2,031.    Elapsed: 0:09:00. Loss: 0.43042\n",
            "  Batch 1,140  of  2,031.    Elapsed: 0:09:02. Loss: 0.43016\n",
            "  Batch 1,145  of  2,031.    Elapsed: 0:09:05. Loss: 0.42982\n",
            "  Batch 1,150  of  2,031.    Elapsed: 0:09:07. Loss: 0.42958\n",
            "  Batch 1,155  of  2,031.    Elapsed: 0:09:09. Loss: 0.42959\n",
            "  Batch 1,160  of  2,031.    Elapsed: 0:09:12. Loss: 0.42940\n",
            "  Batch 1,165  of  2,031.    Elapsed: 0:09:14. Loss: 0.42887\n",
            "  Batch 1,170  of  2,031.    Elapsed: 0:09:17. Loss: 0.42890\n",
            "  Batch 1,175  of  2,031.    Elapsed: 0:09:19. Loss: 0.42915\n",
            "  Batch 1,180  of  2,031.    Elapsed: 0:09:21. Loss: 0.42949\n",
            "  Batch 1,185  of  2,031.    Elapsed: 0:09:24. Loss: 0.42988\n",
            "  Batch 1,190  of  2,031.    Elapsed: 0:09:26. Loss: 0.42995\n",
            "  Batch 1,195  of  2,031.    Elapsed: 0:09:28. Loss: 0.43001\n",
            "  Batch 1,200  of  2,031.    Elapsed: 0:09:31. Loss: 0.43012\n",
            "  Batch 1,205  of  2,031.    Elapsed: 0:09:33. Loss: 0.43023\n",
            "  Batch 1,210  of  2,031.    Elapsed: 0:09:36. Loss: 0.43026\n",
            "  Batch 1,215  of  2,031.    Elapsed: 0:09:38. Loss: 0.43010\n",
            "  Batch 1,220  of  2,031.    Elapsed: 0:09:40. Loss: 0.43000\n",
            "  Batch 1,225  of  2,031.    Elapsed: 0:09:43. Loss: 0.43021\n",
            "  Batch 1,230  of  2,031.    Elapsed: 0:09:45. Loss: 0.43011\n",
            "  Batch 1,235  of  2,031.    Elapsed: 0:09:47. Loss: 0.43022\n",
            "  Batch 1,240  of  2,031.    Elapsed: 0:09:50. Loss: 0.43035\n",
            "  Batch 1,245  of  2,031.    Elapsed: 0:09:52. Loss: 0.43039\n",
            "  Batch 1,250  of  2,031.    Elapsed: 0:09:55. Loss: 0.43020\n",
            "  Batch 1,255  of  2,031.    Elapsed: 0:09:57. Loss: 0.43007\n",
            "  Batch 1,260  of  2,031.    Elapsed: 0:09:59. Loss: 0.42971\n",
            "  Batch 1,265  of  2,031.    Elapsed: 0:10:02. Loss: 0.42986\n",
            "  Batch 1,270  of  2,031.    Elapsed: 0:10:04. Loss: 0.42996\n",
            "  Batch 1,275  of  2,031.    Elapsed: 0:10:06. Loss: 0.42978\n",
            "  Batch 1,280  of  2,031.    Elapsed: 0:10:09. Loss: 0.42972\n",
            "  Batch 1,285  of  2,031.    Elapsed: 0:10:11. Loss: 0.42949\n",
            "  Batch 1,290  of  2,031.    Elapsed: 0:10:14. Loss: 0.42983\n",
            "  Batch 1,295  of  2,031.    Elapsed: 0:10:16. Loss: 0.43004\n",
            "  Batch 1,300  of  2,031.    Elapsed: 0:10:18. Loss: 0.42959\n",
            "  Batch 1,305  of  2,031.    Elapsed: 0:10:21. Loss: 0.42970\n",
            "  Batch 1,310  of  2,031.    Elapsed: 0:10:23. Loss: 0.42973\n",
            "  Batch 1,315  of  2,031.    Elapsed: 0:10:25. Loss: 0.42947\n",
            "  Batch 1,320  of  2,031.    Elapsed: 0:10:28. Loss: 0.42943\n",
            "  Batch 1,325  of  2,031.    Elapsed: 0:10:30. Loss: 0.42964\n",
            "  Batch 1,330  of  2,031.    Elapsed: 0:10:33. Loss: 0.42966\n",
            "  Batch 1,335  of  2,031.    Elapsed: 0:10:35. Loss: 0.42973\n",
            "  Batch 1,340  of  2,031.    Elapsed: 0:10:37. Loss: 0.42966\n",
            "  Batch 1,345  of  2,031.    Elapsed: 0:10:40. Loss: 0.42972\n",
            "  Batch 1,350  of  2,031.    Elapsed: 0:10:42. Loss: 0.42974\n",
            "  Batch 1,355  of  2,031.    Elapsed: 0:10:45. Loss: 0.42969\n",
            "  Batch 1,360  of  2,031.    Elapsed: 0:10:47. Loss: 0.42957\n",
            "  Batch 1,365  of  2,031.    Elapsed: 0:10:49. Loss: 0.42954\n",
            "  Batch 1,370  of  2,031.    Elapsed: 0:10:52. Loss: 0.42945\n",
            "  Batch 1,375  of  2,031.    Elapsed: 0:10:54. Loss: 0.42946\n",
            "  Batch 1,380  of  2,031.    Elapsed: 0:10:56. Loss: 0.42930\n",
            "  Batch 1,385  of  2,031.    Elapsed: 0:10:59. Loss: 0.42918\n",
            "  Batch 1,390  of  2,031.    Elapsed: 0:11:01. Loss: 0.42913\n",
            "  Batch 1,395  of  2,031.    Elapsed: 0:11:04. Loss: 0.42931\n",
            "  Batch 1,400  of  2,031.    Elapsed: 0:11:06. Loss: 0.42918\n",
            "  Batch 1,405  of  2,031.    Elapsed: 0:11:08. Loss: 0.42921\n",
            "  Batch 1,410  of  2,031.    Elapsed: 0:11:11. Loss: 0.42907\n",
            "  Batch 1,415  of  2,031.    Elapsed: 0:11:13. Loss: 0.42906\n",
            "  Batch 1,420  of  2,031.    Elapsed: 0:11:15. Loss: 0.42915\n",
            "  Batch 1,425  of  2,031.    Elapsed: 0:11:18. Loss: 0.42907\n",
            "  Batch 1,430  of  2,031.    Elapsed: 0:11:20. Loss: 0.42908\n",
            "  Batch 1,435  of  2,031.    Elapsed: 0:11:23. Loss: 0.42903\n",
            "  Batch 1,440  of  2,031.    Elapsed: 0:11:25. Loss: 0.42910\n",
            "  Batch 1,445  of  2,031.    Elapsed: 0:11:27. Loss: 0.42882\n",
            "  Batch 1,450  of  2,031.    Elapsed: 0:11:30. Loss: 0.42888\n",
            "  Batch 1,455  of  2,031.    Elapsed: 0:11:32. Loss: 0.42890\n",
            "  Batch 1,460  of  2,031.    Elapsed: 0:11:34. Loss: 0.42881\n",
            "  Batch 1,465  of  2,031.    Elapsed: 0:11:37. Loss: 0.42865\n",
            "  Batch 1,470  of  2,031.    Elapsed: 0:11:39. Loss: 0.42871\n",
            "  Batch 1,475  of  2,031.    Elapsed: 0:11:42. Loss: 0.42889\n",
            "  Batch 1,480  of  2,031.    Elapsed: 0:11:44. Loss: 0.42904\n",
            "  Batch 1,485  of  2,031.    Elapsed: 0:11:46. Loss: 0.42890\n",
            "  Batch 1,490  of  2,031.    Elapsed: 0:11:49. Loss: 0.42893\n",
            "  Batch 1,495  of  2,031.    Elapsed: 0:11:51. Loss: 0.42878\n",
            "  Batch 1,500  of  2,031.    Elapsed: 0:11:53. Loss: 0.42885\n",
            "  Batch 1,505  of  2,031.    Elapsed: 0:11:56. Loss: 0.42876\n",
            "  Batch 1,510  of  2,031.    Elapsed: 0:11:58. Loss: 0.42881\n",
            "  Batch 1,515  of  2,031.    Elapsed: 0:12:01. Loss: 0.42849\n",
            "  Batch 1,520  of  2,031.    Elapsed: 0:12:03. Loss: 0.42845\n",
            "  Batch 1,525  of  2,031.    Elapsed: 0:12:05. Loss: 0.42855\n",
            "  Batch 1,530  of  2,031.    Elapsed: 0:12:08. Loss: 0.42852\n",
            "  Batch 1,535  of  2,031.    Elapsed: 0:12:10. Loss: 0.42827\n",
            "  Batch 1,540  of  2,031.    Elapsed: 0:12:12. Loss: 0.42802\n",
            "  Batch 1,545  of  2,031.    Elapsed: 0:12:15. Loss: 0.42815\n",
            "  Batch 1,550  of  2,031.    Elapsed: 0:12:17. Loss: 0.42809\n",
            "  Batch 1,555  of  2,031.    Elapsed: 0:12:20. Loss: 0.42781\n",
            "  Batch 1,560  of  2,031.    Elapsed: 0:12:22. Loss: 0.42791\n",
            "  Batch 1,565  of  2,031.    Elapsed: 0:12:24. Loss: 0.42758\n",
            "  Batch 1,570  of  2,031.    Elapsed: 0:12:27. Loss: 0.42735\n",
            "  Batch 1,575  of  2,031.    Elapsed: 0:12:29. Loss: 0.42759\n",
            "  Batch 1,580  of  2,031.    Elapsed: 0:12:32. Loss: 0.42770\n",
            "  Batch 1,585  of  2,031.    Elapsed: 0:12:34. Loss: 0.42782\n",
            "  Batch 1,590  of  2,031.    Elapsed: 0:12:36. Loss: 0.42773\n",
            "  Batch 1,595  of  2,031.    Elapsed: 0:12:39. Loss: 0.42761\n",
            "  Batch 1,600  of  2,031.    Elapsed: 0:12:41. Loss: 0.42773\n",
            "  Batch 1,605  of  2,031.    Elapsed: 0:12:43. Loss: 0.42782\n",
            "  Batch 1,610  of  2,031.    Elapsed: 0:12:46. Loss: 0.42776\n",
            "  Batch 1,615  of  2,031.    Elapsed: 0:12:48. Loss: 0.42779\n",
            "  Batch 1,620  of  2,031.    Elapsed: 0:12:51. Loss: 0.42777\n",
            "  Batch 1,625  of  2,031.    Elapsed: 0:12:53. Loss: 0.42783\n",
            "  Batch 1,630  of  2,031.    Elapsed: 0:12:55. Loss: 0.42769\n",
            "  Batch 1,635  of  2,031.    Elapsed: 0:12:58. Loss: 0.42790\n",
            "  Batch 1,640  of  2,031.    Elapsed: 0:13:00. Loss: 0.42825\n",
            "  Batch 1,645  of  2,031.    Elapsed: 0:13:02. Loss: 0.42831\n",
            "  Batch 1,650  of  2,031.    Elapsed: 0:13:05. Loss: 0.42827\n",
            "  Batch 1,655  of  2,031.    Elapsed: 0:13:07. Loss: 0.42834\n",
            "  Batch 1,660  of  2,031.    Elapsed: 0:13:10. Loss: 0.42853\n",
            "  Batch 1,665  of  2,031.    Elapsed: 0:13:12. Loss: 0.42836\n",
            "  Batch 1,670  of  2,031.    Elapsed: 0:13:14. Loss: 0.42835\n",
            "  Batch 1,675  of  2,031.    Elapsed: 0:13:17. Loss: 0.42834\n",
            "  Batch 1,680  of  2,031.    Elapsed: 0:13:19. Loss: 0.42829\n",
            "  Batch 1,685  of  2,031.    Elapsed: 0:13:21. Loss: 0.42810\n",
            "  Batch 1,690  of  2,031.    Elapsed: 0:13:24. Loss: 0.42808\n",
            "  Batch 1,695  of  2,031.    Elapsed: 0:13:26. Loss: 0.42790\n",
            "  Batch 1,700  of  2,031.    Elapsed: 0:13:29. Loss: 0.42767\n",
            "  Batch 1,705  of  2,031.    Elapsed: 0:13:31. Loss: 0.42743\n",
            "  Batch 1,710  of  2,031.    Elapsed: 0:13:33. Loss: 0.42749\n",
            "  Batch 1,715  of  2,031.    Elapsed: 0:13:36. Loss: 0.42768\n",
            "  Batch 1,720  of  2,031.    Elapsed: 0:13:38. Loss: 0.42753\n",
            "  Batch 1,725  of  2,031.    Elapsed: 0:13:40. Loss: 0.42744\n",
            "  Batch 1,730  of  2,031.    Elapsed: 0:13:43. Loss: 0.42751\n",
            "  Batch 1,735  of  2,031.    Elapsed: 0:13:45. Loss: 0.42744\n",
            "  Batch 1,740  of  2,031.    Elapsed: 0:13:48. Loss: 0.42742\n",
            "  Batch 1,745  of  2,031.    Elapsed: 0:13:50. Loss: 0.42742\n",
            "  Batch 1,750  of  2,031.    Elapsed: 0:13:52. Loss: 0.42730\n",
            "  Batch 1,755  of  2,031.    Elapsed: 0:13:55. Loss: 0.42735\n",
            "  Batch 1,760  of  2,031.    Elapsed: 0:13:57. Loss: 0.42739\n",
            "  Batch 1,765  of  2,031.    Elapsed: 0:14:00. Loss: 0.42732\n",
            "  Batch 1,770  of  2,031.    Elapsed: 0:14:02. Loss: 0.42725\n",
            "  Batch 1,775  of  2,031.    Elapsed: 0:14:04. Loss: 0.42724\n",
            "  Batch 1,780  of  2,031.    Elapsed: 0:14:07. Loss: 0.42736\n",
            "  Batch 1,785  of  2,031.    Elapsed: 0:14:09. Loss: 0.42741\n",
            "  Batch 1,790  of  2,031.    Elapsed: 0:14:11. Loss: 0.42716\n",
            "  Batch 1,795  of  2,031.    Elapsed: 0:14:14. Loss: 0.42710\n",
            "  Batch 1,800  of  2,031.    Elapsed: 0:14:16. Loss: 0.42690\n",
            "  Batch 1,805  of  2,031.    Elapsed: 0:14:19. Loss: 0.42695\n",
            "  Batch 1,810  of  2,031.    Elapsed: 0:14:21. Loss: 0.42689\n",
            "  Batch 1,815  of  2,031.    Elapsed: 0:14:23. Loss: 0.42687\n",
            "  Batch 1,820  of  2,031.    Elapsed: 0:14:26. Loss: 0.42669\n",
            "  Batch 1,825  of  2,031.    Elapsed: 0:14:28. Loss: 0.42658\n",
            "  Batch 1,830  of  2,031.    Elapsed: 0:14:30. Loss: 0.42654\n",
            "  Batch 1,835  of  2,031.    Elapsed: 0:14:33. Loss: 0.42655\n",
            "  Batch 1,840  of  2,031.    Elapsed: 0:14:35. Loss: 0.42650\n",
            "  Batch 1,845  of  2,031.    Elapsed: 0:14:38. Loss: 0.42642\n",
            "  Batch 1,850  of  2,031.    Elapsed: 0:14:40. Loss: 0.42629\n",
            "  Batch 1,855  of  2,031.    Elapsed: 0:14:42. Loss: 0.42645\n",
            "  Batch 1,860  of  2,031.    Elapsed: 0:14:45. Loss: 0.42665\n",
            "  Batch 1,865  of  2,031.    Elapsed: 0:14:47. Loss: 0.42678\n",
            "  Batch 1,870  of  2,031.    Elapsed: 0:14:49. Loss: 0.42695\n",
            "  Batch 1,875  of  2,031.    Elapsed: 0:14:52. Loss: 0.42691\n",
            "  Batch 1,880  of  2,031.    Elapsed: 0:14:54. Loss: 0.42695\n",
            "  Batch 1,885  of  2,031.    Elapsed: 0:14:57. Loss: 0.42689\n",
            "  Batch 1,890  of  2,031.    Elapsed: 0:14:59. Loss: 0.42694\n",
            "  Batch 1,895  of  2,031.    Elapsed: 0:15:01. Loss: 0.42688\n",
            "  Batch 1,900  of  2,031.    Elapsed: 0:15:04. Loss: 0.42683\n",
            "  Batch 1,905  of  2,031.    Elapsed: 0:15:06. Loss: 0.42678\n",
            "  Batch 1,910  of  2,031.    Elapsed: 0:15:08. Loss: 0.42677\n",
            "  Batch 1,915  of  2,031.    Elapsed: 0:15:11. Loss: 0.42675\n",
            "  Batch 1,920  of  2,031.    Elapsed: 0:15:13. Loss: 0.42681\n",
            "  Batch 1,925  of  2,031.    Elapsed: 0:15:16. Loss: 0.42685\n",
            "  Batch 1,930  of  2,031.    Elapsed: 0:15:18. Loss: 0.42686\n",
            "  Batch 1,935  of  2,031.    Elapsed: 0:15:20. Loss: 0.42686\n",
            "  Batch 1,940  of  2,031.    Elapsed: 0:15:23. Loss: 0.42691\n",
            "  Batch 1,945  of  2,031.    Elapsed: 0:15:25. Loss: 0.42700\n",
            "  Batch 1,950  of  2,031.    Elapsed: 0:15:27. Loss: 0.42692\n",
            "  Batch 1,955  of  2,031.    Elapsed: 0:15:30. Loss: 0.42673\n",
            "  Batch 1,960  of  2,031.    Elapsed: 0:15:32. Loss: 0.42681\n",
            "  Batch 1,965  of  2,031.    Elapsed: 0:15:35. Loss: 0.42669\n",
            "  Batch 1,970  of  2,031.    Elapsed: 0:15:37. Loss: 0.42677\n",
            "  Batch 1,975  of  2,031.    Elapsed: 0:15:39. Loss: 0.42660\n",
            "  Batch 1,980  of  2,031.    Elapsed: 0:15:42. Loss: 0.42667\n",
            "  Batch 1,985  of  2,031.    Elapsed: 0:15:44. Loss: 0.42667\n",
            "  Batch 1,990  of  2,031.    Elapsed: 0:15:46. Loss: 0.42675\n",
            "  Batch 1,995  of  2,031.    Elapsed: 0:15:49. Loss: 0.42681\n",
            "  Batch 2,000  of  2,031.    Elapsed: 0:15:51. Loss: 0.42692\n",
            "  Batch 2,005  of  2,031.    Elapsed: 0:15:54. Loss: 0.42701\n",
            "  Batch 2,010  of  2,031.    Elapsed: 0:15:56. Loss: 0.42703\n",
            "  Batch 2,015  of  2,031.    Elapsed: 0:15:58. Loss: 0.42707\n",
            "  Batch 2,020  of  2,031.    Elapsed: 0:16:01. Loss: 0.42701\n",
            "  Batch 2,025  of  2,031.    Elapsed: 0:16:03. Loss: 0.42696\n",
            "  Batch 2,030  of  2,031.    Elapsed: 0:16:05. Loss: 0.42701\n",
            "\n",
            "  Average training loss: 0.43\n",
            "  Training epcoh took: 0:16:06\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.40\n",
            "  Validation took: 0:01:10\n",
            "  Accuracy: 0.88\n",
            "  Accuracy: 0.92\n",
            "  Accuracy: 0.93\n",
            "  Accuracy: 0.85\n",
            "  Accuracy: 0.57\n",
            "  Macro F1-score: 0.47\n",
            "  Macro F1-score: 0.48\n",
            "  Macro F1-score: 0.48\n",
            "  Macro F1-score: 0.46\n",
            "  Macro F1-score: 0.37\n",
            "  Weighted F1-score: 0.82\n",
            "  Weighted F1-score: 0.89\n",
            "  Weighted F1-score: 0.89\n",
            "  Weighted F1-score: 0.78\n",
            "  Weighted F1-score: 0.42\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      1.00      0.94      3570\n",
            "         1.0       0.00      0.00      0.00       491\n",
            "\n",
            "    accuracy                           0.88      4061\n",
            "   macro avg       0.44      0.50      0.47      4061\n",
            "weighted avg       0.77      0.88      0.82      4061\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      1.00      0.96      3751\n",
            "         1.0       0.00      0.00      0.00       310\n",
            "\n",
            "    accuracy                           0.92      4061\n",
            "   macro avg       0.46      0.50      0.48      4061\n",
            "weighted avg       0.85      0.92      0.89      4061\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      1.00      0.96      3762\n",
            "         1.0       0.00      0.00      0.00       299\n",
            "\n",
            "    accuracy                           0.93      4061\n",
            "   macro avg       0.46      0.50      0.48      4061\n",
            "weighted avg       0.86      0.93      0.89      4061\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      1.00      0.92      3435\n",
            "         1.0       0.00      0.00      0.00       626\n",
            "\n",
            "    accuracy                           0.85      4061\n",
            "   macro avg       0.42      0.50      0.46      4061\n",
            "weighted avg       0.72      0.85      0.78      4061\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00      1726\n",
            "         1.0       0.57      1.00      0.73      2335\n",
            "\n",
            "    accuracy                           0.57      4061\n",
            "   macro avg       0.29      0.50      0.37      4061\n",
            "weighted avg       0.33      0.57      0.42      4061\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3570    0]\n",
            " [ 491    0]]\n",
            "[[3751    0]\n",
            " [ 310    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch     5  of  2,031.    Elapsed: 0:00:02. Loss: 0.43767\n",
            "  Batch    10  of  2,031.    Elapsed: 0:00:05. Loss: 0.42197\n",
            "  Batch    15  of  2,031.    Elapsed: 0:00:07. Loss: 0.41264\n",
            "  Batch    20  of  2,031.    Elapsed: 0:00:10. Loss: 0.42716\n",
            "  Batch    25  of  2,031.    Elapsed: 0:00:12. Loss: 0.42196\n",
            "  Batch    30  of  2,031.    Elapsed: 0:00:14. Loss: 0.41928\n",
            "  Batch    35  of  2,031.    Elapsed: 0:00:17. Loss: 0.42125\n",
            "  Batch    40  of  2,031.    Elapsed: 0:00:19. Loss: 0.42800\n",
            "  Batch    45  of  2,031.    Elapsed: 0:00:21. Loss: 0.42723\n",
            "  Batch    50  of  2,031.    Elapsed: 0:00:24. Loss: 0.43017\n",
            "  Batch    55  of  2,031.    Elapsed: 0:00:26. Loss: 0.43227\n",
            "  Batch    60  of  2,031.    Elapsed: 0:00:29. Loss: 0.42912\n",
            "  Batch    65  of  2,031.    Elapsed: 0:00:31. Loss: 0.42915\n",
            "  Batch    70  of  2,031.    Elapsed: 0:00:33. Loss: 0.42921\n",
            "  Batch    75  of  2,031.    Elapsed: 0:00:36. Loss: 0.43356\n",
            "  Batch    80  of  2,031.    Elapsed: 0:00:38. Loss: 0.43566\n",
            "  Batch    85  of  2,031.    Elapsed: 0:00:40. Loss: 0.43596\n",
            "  Batch    90  of  2,031.    Elapsed: 0:00:43. Loss: 0.43244\n",
            "  Batch    95  of  2,031.    Elapsed: 0:00:45. Loss: 0.43238\n",
            "  Batch   100  of  2,031.    Elapsed: 0:00:48. Loss: 0.42903\n",
            "  Batch   105  of  2,031.    Elapsed: 0:00:50. Loss: 0.43034\n",
            "  Batch   110  of  2,031.    Elapsed: 0:00:52. Loss: 0.43022\n",
            "  Batch   115  of  2,031.    Elapsed: 0:00:55. Loss: 0.42926\n",
            "  Batch   120  of  2,031.    Elapsed: 0:00:57. Loss: 0.42875\n",
            "  Batch   125  of  2,031.    Elapsed: 0:01:00. Loss: 0.42897\n",
            "  Batch   130  of  2,031.    Elapsed: 0:01:02. Loss: 0.42856\n",
            "  Batch   135  of  2,031.    Elapsed: 0:01:04. Loss: 0.42885\n",
            "  Batch   140  of  2,031.    Elapsed: 0:01:07. Loss: 0.42608\n",
            "  Batch   145  of  2,031.    Elapsed: 0:01:09. Loss: 0.42593\n",
            "  Batch   150  of  2,031.    Elapsed: 0:01:11. Loss: 0.42569\n",
            "  Batch   155  of  2,031.    Elapsed: 0:01:14. Loss: 0.42163\n",
            "  Batch   160  of  2,031.    Elapsed: 0:01:16. Loss: 0.42149\n",
            "  Batch   165  of  2,031.    Elapsed: 0:01:19. Loss: 0.41954\n",
            "  Batch   170  of  2,031.    Elapsed: 0:01:21. Loss: 0.41968\n",
            "  Batch   175  of  2,031.    Elapsed: 0:01:23. Loss: 0.41927\n",
            "  Batch   180  of  2,031.    Elapsed: 0:01:26. Loss: 0.42125\n",
            "  Batch   185  of  2,031.    Elapsed: 0:01:28. Loss: 0.42026\n",
            "  Batch   190  of  2,031.    Elapsed: 0:01:30. Loss: 0.42121\n",
            "  Batch   195  of  2,031.    Elapsed: 0:01:33. Loss: 0.41989\n",
            "  Batch   200  of  2,031.    Elapsed: 0:01:35. Loss: 0.41920\n",
            "  Batch   205  of  2,031.    Elapsed: 0:01:38. Loss: 0.41989\n",
            "  Batch   210  of  2,031.    Elapsed: 0:01:40. Loss: 0.41950\n",
            "  Batch   215  of  2,031.    Elapsed: 0:01:42. Loss: 0.42029\n",
            "  Batch   220  of  2,031.    Elapsed: 0:01:45. Loss: 0.41986\n",
            "  Batch   225  of  2,031.    Elapsed: 0:01:47. Loss: 0.41974\n",
            "  Batch   230  of  2,031.    Elapsed: 0:01:49. Loss: 0.42078\n",
            "  Batch   235  of  2,031.    Elapsed: 0:01:52. Loss: 0.42096\n",
            "  Batch   240  of  2,031.    Elapsed: 0:01:54. Loss: 0.42156\n",
            "  Batch   245  of  2,031.    Elapsed: 0:01:57. Loss: 0.42212\n",
            "  Batch   250  of  2,031.    Elapsed: 0:01:59. Loss: 0.42209\n",
            "  Batch   255  of  2,031.    Elapsed: 0:02:01. Loss: 0.42221\n",
            "  Batch   260  of  2,031.    Elapsed: 0:02:04. Loss: 0.42193\n",
            "  Batch   265  of  2,031.    Elapsed: 0:02:06. Loss: 0.42215\n",
            "  Batch   270  of  2,031.    Elapsed: 0:02:09. Loss: 0.42232\n",
            "  Batch   275  of  2,031.    Elapsed: 0:02:11. Loss: 0.42152\n",
            "  Batch   280  of  2,031.    Elapsed: 0:02:13. Loss: 0.42281\n",
            "  Batch   285  of  2,031.    Elapsed: 0:02:16. Loss: 0.42381\n",
            "  Batch   290  of  2,031.    Elapsed: 0:02:18. Loss: 0.42365\n",
            "  Batch   295  of  2,031.    Elapsed: 0:02:20. Loss: 0.42355\n",
            "  Batch   300  of  2,031.    Elapsed: 0:02:23. Loss: 0.42369\n",
            "  Batch   305  of  2,031.    Elapsed: 0:02:25. Loss: 0.42428\n",
            "  Batch   310  of  2,031.    Elapsed: 0:02:28. Loss: 0.42435\n",
            "  Batch   315  of  2,031.    Elapsed: 0:02:30. Loss: 0.42377\n",
            "  Batch   320  of  2,031.    Elapsed: 0:02:32. Loss: 0.42432\n",
            "  Batch   325  of  2,031.    Elapsed: 0:02:35. Loss: 0.42423\n",
            "  Batch   330  of  2,031.    Elapsed: 0:02:37. Loss: 0.42378\n",
            "  Batch   335  of  2,031.    Elapsed: 0:02:39. Loss: 0.42353\n",
            "  Batch   340  of  2,031.    Elapsed: 0:02:42. Loss: 0.42331\n",
            "  Batch   345  of  2,031.    Elapsed: 0:02:44. Loss: 0.42350\n",
            "  Batch   350  of  2,031.    Elapsed: 0:02:47. Loss: 0.42386\n",
            "  Batch   355  of  2,031.    Elapsed: 0:02:49. Loss: 0.42471\n",
            "  Batch   360  of  2,031.    Elapsed: 0:02:51. Loss: 0.42424\n",
            "  Batch   365  of  2,031.    Elapsed: 0:02:54. Loss: 0.42504\n",
            "  Batch   370  of  2,031.    Elapsed: 0:02:56. Loss: 0.42449\n",
            "  Batch   375  of  2,031.    Elapsed: 0:02:58. Loss: 0.42505\n",
            "  Batch   380  of  2,031.    Elapsed: 0:03:01. Loss: 0.42505\n",
            "  Batch   385  of  2,031.    Elapsed: 0:03:03. Loss: 0.42438\n",
            "  Batch   390  of  2,031.    Elapsed: 0:03:06. Loss: 0.42438\n",
            "  Batch   395  of  2,031.    Elapsed: 0:03:08. Loss: 0.42444\n",
            "  Batch   400  of  2,031.    Elapsed: 0:03:10. Loss: 0.42509\n",
            "  Batch   405  of  2,031.    Elapsed: 0:03:13. Loss: 0.42525\n",
            "  Batch   410  of  2,031.    Elapsed: 0:03:15. Loss: 0.42511\n",
            "  Batch   415  of  2,031.    Elapsed: 0:03:17. Loss: 0.42434\n",
            "  Batch   420  of  2,031.    Elapsed: 0:03:20. Loss: 0.42409\n",
            "  Batch   425  of  2,031.    Elapsed: 0:03:22. Loss: 0.42331\n",
            "  Batch   430  of  2,031.    Elapsed: 0:03:25. Loss: 0.42324\n",
            "  Batch   435  of  2,031.    Elapsed: 0:03:27. Loss: 0.42363\n",
            "  Batch   440  of  2,031.    Elapsed: 0:03:29. Loss: 0.42312\n",
            "  Batch   445  of  2,031.    Elapsed: 0:03:32. Loss: 0.42309\n",
            "  Batch   450  of  2,031.    Elapsed: 0:03:34. Loss: 0.42275\n",
            "  Batch   455  of  2,031.    Elapsed: 0:03:37. Loss: 0.42276\n",
            "  Batch   460  of  2,031.    Elapsed: 0:03:39. Loss: 0.42219\n",
            "  Batch   465  of  2,031.    Elapsed: 0:03:41. Loss: 0.42205\n",
            "  Batch   470  of  2,031.    Elapsed: 0:03:44. Loss: 0.42286\n",
            "  Batch   475  of  2,031.    Elapsed: 0:03:46. Loss: 0.42258\n",
            "  Batch   480  of  2,031.    Elapsed: 0:03:48. Loss: 0.42201\n",
            "  Batch   485  of  2,031.    Elapsed: 0:03:51. Loss: 0.42282\n",
            "  Batch   490  of  2,031.    Elapsed: 0:03:53. Loss: 0.42314\n",
            "  Batch   495  of  2,031.    Elapsed: 0:03:56. Loss: 0.42351\n",
            "  Batch   500  of  2,031.    Elapsed: 0:03:58. Loss: 0.42290\n",
            "  Batch   505  of  2,031.    Elapsed: 0:04:00. Loss: 0.42326\n",
            "  Batch   510  of  2,031.    Elapsed: 0:04:03. Loss: 0.42326\n",
            "  Batch   515  of  2,031.    Elapsed: 0:04:05. Loss: 0.42375\n",
            "  Batch   520  of  2,031.    Elapsed: 0:04:07. Loss: 0.42411\n",
            "  Batch   525  of  2,031.    Elapsed: 0:04:10. Loss: 0.42392\n",
            "  Batch   530  of  2,031.    Elapsed: 0:04:12. Loss: 0.42344\n",
            "  Batch   535  of  2,031.    Elapsed: 0:04:14. Loss: 0.42372\n",
            "  Batch   540  of  2,031.    Elapsed: 0:04:17. Loss: 0.42469\n",
            "  Batch   545  of  2,031.    Elapsed: 0:04:19. Loss: 0.42429\n",
            "  Batch   550  of  2,031.    Elapsed: 0:04:22. Loss: 0.42408\n",
            "  Batch   555  of  2,031.    Elapsed: 0:04:24. Loss: 0.42397\n",
            "  Batch   560  of  2,031.    Elapsed: 0:04:26. Loss: 0.42370\n",
            "  Batch   565  of  2,031.    Elapsed: 0:04:29. Loss: 0.42343\n",
            "  Batch   570  of  2,031.    Elapsed: 0:04:31. Loss: 0.42365\n",
            "  Batch   575  of  2,031.    Elapsed: 0:04:34. Loss: 0.42343\n",
            "  Batch   580  of  2,031.    Elapsed: 0:04:36. Loss: 0.42319\n",
            "  Batch   585  of  2,031.    Elapsed: 0:04:38. Loss: 0.42298\n",
            "  Batch   590  of  2,031.    Elapsed: 0:04:41. Loss: 0.42332\n",
            "  Batch   595  of  2,031.    Elapsed: 0:04:43. Loss: 0.42307\n",
            "  Batch   600  of  2,031.    Elapsed: 0:04:45. Loss: 0.42258\n",
            "  Batch   605  of  2,031.    Elapsed: 0:04:48. Loss: 0.42228\n",
            "  Batch   610  of  2,031.    Elapsed: 0:04:50. Loss: 0.42226\n",
            "  Batch   615  of  2,031.    Elapsed: 0:04:53. Loss: 0.42233\n",
            "  Batch   620  of  2,031.    Elapsed: 0:04:55. Loss: 0.42218\n",
            "  Batch   625  of  2,031.    Elapsed: 0:04:57. Loss: 0.42230\n",
            "  Batch   630  of  2,031.    Elapsed: 0:05:00. Loss: 0.42231\n",
            "  Batch   635  of  2,031.    Elapsed: 0:05:02. Loss: 0.42200\n",
            "  Batch   640  of  2,031.    Elapsed: 0:05:04. Loss: 0.42181\n",
            "  Batch   645  of  2,031.    Elapsed: 0:05:07. Loss: 0.42126\n",
            "  Batch   650  of  2,031.    Elapsed: 0:05:09. Loss: 0.42080\n",
            "  Batch   655  of  2,031.    Elapsed: 0:05:12. Loss: 0.42063\n",
            "  Batch   660  of  2,031.    Elapsed: 0:05:14. Loss: 0.42033\n",
            "  Batch   665  of  2,031.    Elapsed: 0:05:16. Loss: 0.42030\n",
            "  Batch   670  of  2,031.    Elapsed: 0:05:19. Loss: 0.42084\n",
            "  Batch   675  of  2,031.    Elapsed: 0:05:21. Loss: 0.42118\n",
            "  Batch   680  of  2,031.    Elapsed: 0:05:23. Loss: 0.42123\n",
            "  Batch   685  of  2,031.    Elapsed: 0:05:26. Loss: 0.42106\n",
            "  Batch   690  of  2,031.    Elapsed: 0:05:28. Loss: 0.42061\n",
            "  Batch   695  of  2,031.    Elapsed: 0:05:31. Loss: 0.42099\n",
            "  Batch   700  of  2,031.    Elapsed: 0:05:33. Loss: 0.42130\n",
            "  Batch   705  of  2,031.    Elapsed: 0:05:35. Loss: 0.42138\n",
            "  Batch   710  of  2,031.    Elapsed: 0:05:38. Loss: 0.42145\n",
            "  Batch   715  of  2,031.    Elapsed: 0:05:40. Loss: 0.42121\n",
            "  Batch   720  of  2,031.    Elapsed: 0:05:43. Loss: 0.42143\n",
            "  Batch   725  of  2,031.    Elapsed: 0:05:45. Loss: 0.42146\n",
            "  Batch   730  of  2,031.    Elapsed: 0:05:47. Loss: 0.42164\n",
            "  Batch   735  of  2,031.    Elapsed: 0:05:50. Loss: 0.42118\n",
            "  Batch   740  of  2,031.    Elapsed: 0:05:52. Loss: 0.42121\n",
            "  Batch   745  of  2,031.    Elapsed: 0:05:54. Loss: 0.42108\n",
            "  Batch   750  of  2,031.    Elapsed: 0:05:57. Loss: 0.42097\n",
            "  Batch   755  of  2,031.    Elapsed: 0:05:59. Loss: 0.42137\n",
            "  Batch   760  of  2,031.    Elapsed: 0:06:02. Loss: 0.42169\n",
            "  Batch   765  of  2,031.    Elapsed: 0:06:04. Loss: 0.42125\n",
            "  Batch   770  of  2,031.    Elapsed: 0:06:06. Loss: 0.42167\n",
            "  Batch   775  of  2,031.    Elapsed: 0:06:09. Loss: 0.42181\n",
            "  Batch   780  of  2,031.    Elapsed: 0:06:11. Loss: 0.42179\n",
            "  Batch   785  of  2,031.    Elapsed: 0:06:13. Loss: 0.42157\n",
            "  Batch   790  of  2,031.    Elapsed: 0:06:16. Loss: 0.42193\n",
            "  Batch   795  of  2,031.    Elapsed: 0:06:18. Loss: 0.42168\n",
            "  Batch   800  of  2,031.    Elapsed: 0:06:21. Loss: 0.42199\n",
            "  Batch   805  of  2,031.    Elapsed: 0:06:23. Loss: 0.42193\n",
            "  Batch   810  of  2,031.    Elapsed: 0:06:25. Loss: 0.42176\n",
            "  Batch   815  of  2,031.    Elapsed: 0:06:28. Loss: 0.42172\n",
            "  Batch   820  of  2,031.    Elapsed: 0:06:30. Loss: 0.42196\n",
            "  Batch   825  of  2,031.    Elapsed: 0:06:33. Loss: 0.42216\n",
            "  Batch   830  of  2,031.    Elapsed: 0:06:35. Loss: 0.42188\n",
            "  Batch   835  of  2,031.    Elapsed: 0:06:37. Loss: 0.42186\n",
            "  Batch   840  of  2,031.    Elapsed: 0:06:40. Loss: 0.42181\n",
            "  Batch   845  of  2,031.    Elapsed: 0:06:42. Loss: 0.42170\n",
            "  Batch   850  of  2,031.    Elapsed: 0:06:44. Loss: 0.42142\n",
            "  Batch   855  of  2,031.    Elapsed: 0:06:47. Loss: 0.42152\n",
            "  Batch   860  of  2,031.    Elapsed: 0:06:49. Loss: 0.42120\n",
            "  Batch   865  of  2,031.    Elapsed: 0:06:52. Loss: 0.42127\n",
            "  Batch   870  of  2,031.    Elapsed: 0:06:54. Loss: 0.42105\n",
            "  Batch   875  of  2,031.    Elapsed: 0:06:56. Loss: 0.42094\n",
            "  Batch   880  of  2,031.    Elapsed: 0:06:59. Loss: 0.42059\n",
            "  Batch   885  of  2,031.    Elapsed: 0:07:01. Loss: 0.42034\n",
            "  Batch   890  of  2,031.    Elapsed: 0:07:03. Loss: 0.42046\n",
            "  Batch   895  of  2,031.    Elapsed: 0:07:06. Loss: 0.42062\n",
            "  Batch   900  of  2,031.    Elapsed: 0:07:08. Loss: 0.42050\n",
            "  Batch   905  of  2,031.    Elapsed: 0:07:11. Loss: 0.42055\n",
            "  Batch   910  of  2,031.    Elapsed: 0:07:13. Loss: 0.42052\n",
            "  Batch   915  of  2,031.    Elapsed: 0:07:15. Loss: 0.42002\n",
            "  Batch   920  of  2,031.    Elapsed: 0:07:18. Loss: 0.42008\n",
            "  Batch   925  of  2,031.    Elapsed: 0:07:20. Loss: 0.41984\n",
            "  Batch   930  of  2,031.    Elapsed: 0:07:22. Loss: 0.41946\n",
            "  Batch   935  of  2,031.    Elapsed: 0:07:25. Loss: 0.41917\n",
            "  Batch   940  of  2,031.    Elapsed: 0:07:27. Loss: 0.41930\n",
            "  Batch   945  of  2,031.    Elapsed: 0:07:30. Loss: 0.41965\n",
            "  Batch   950  of  2,031.    Elapsed: 0:07:32. Loss: 0.41987\n",
            "  Batch   955  of  2,031.    Elapsed: 0:07:34. Loss: 0.41989\n",
            "  Batch   960  of  2,031.    Elapsed: 0:07:37. Loss: 0.42003\n",
            "  Batch   965  of  2,031.    Elapsed: 0:07:39. Loss: 0.42003\n",
            "  Batch   970  of  2,031.    Elapsed: 0:07:41. Loss: 0.42044\n",
            "  Batch   975  of  2,031.    Elapsed: 0:07:44. Loss: 0.42027\n",
            "  Batch   980  of  2,031.    Elapsed: 0:07:46. Loss: 0.41974\n",
            "  Batch   985  of  2,031.    Elapsed: 0:07:49. Loss: 0.41944\n",
            "  Batch   990  of  2,031.    Elapsed: 0:07:51. Loss: 0.41940\n",
            "  Batch   995  of  2,031.    Elapsed: 0:07:53. Loss: 0.41931\n",
            "  Batch 1,000  of  2,031.    Elapsed: 0:07:56. Loss: 0.41939\n",
            "  Batch 1,005  of  2,031.    Elapsed: 0:07:58. Loss: 0.41948\n",
            "  Batch 1,010  of  2,031.    Elapsed: 0:08:00. Loss: 0.41913\n",
            "  Batch 1,015  of  2,031.    Elapsed: 0:08:03. Loss: 0.41920\n",
            "  Batch 1,020  of  2,031.    Elapsed: 0:08:05. Loss: 0.41926\n",
            "  Batch 1,025  of  2,031.    Elapsed: 0:08:08. Loss: 0.41909\n",
            "  Batch 1,030  of  2,031.    Elapsed: 0:08:10. Loss: 0.41902\n",
            "  Batch 1,035  of  2,031.    Elapsed: 0:08:12. Loss: 0.41894\n",
            "  Batch 1,040  of  2,031.    Elapsed: 0:08:15. Loss: 0.41927\n",
            "  Batch 1,045  of  2,031.    Elapsed: 0:08:17. Loss: 0.41957\n",
            "  Batch 1,050  of  2,031.    Elapsed: 0:08:19. Loss: 0.41951\n",
            "  Batch 1,055  of  2,031.    Elapsed: 0:08:22. Loss: 0.41956\n",
            "  Batch 1,060  of  2,031.    Elapsed: 0:08:24. Loss: 0.41946\n",
            "  Batch 1,065  of  2,031.    Elapsed: 0:08:27. Loss: 0.41908\n",
            "  Batch 1,070  of  2,031.    Elapsed: 0:08:29. Loss: 0.41894\n",
            "  Batch 1,075  of  2,031.    Elapsed: 0:08:31. Loss: 0.41931\n",
            "  Batch 1,080  of  2,031.    Elapsed: 0:08:34. Loss: 0.41909\n",
            "  Batch 1,085  of  2,031.    Elapsed: 0:08:36. Loss: 0.41926\n",
            "  Batch 1,090  of  2,031.    Elapsed: 0:08:38. Loss: 0.41922\n",
            "  Batch 1,095  of  2,031.    Elapsed: 0:08:41. Loss: 0.41929\n",
            "  Batch 1,100  of  2,031.    Elapsed: 0:08:43. Loss: 0.41919\n",
            "  Batch 1,105  of  2,031.    Elapsed: 0:08:46. Loss: 0.41901\n",
            "  Batch 1,110  of  2,031.    Elapsed: 0:08:48. Loss: 0.41916\n",
            "  Batch 1,115  of  2,031.    Elapsed: 0:08:50. Loss: 0.41936\n",
            "  Batch 1,120  of  2,031.    Elapsed: 0:08:53. Loss: 0.41931\n",
            "  Batch 1,125  of  2,031.    Elapsed: 0:08:55. Loss: 0.41976\n",
            "  Batch 1,130  of  2,031.    Elapsed: 0:08:57. Loss: 0.41980\n",
            "  Batch 1,135  of  2,031.    Elapsed: 0:09:00. Loss: 0.42002\n",
            "  Batch 1,140  of  2,031.    Elapsed: 0:09:02. Loss: 0.42029\n",
            "  Batch 1,145  of  2,031.    Elapsed: 0:09:05. Loss: 0.42032\n",
            "  Batch 1,150  of  2,031.    Elapsed: 0:09:07. Loss: 0.42059\n",
            "  Batch 1,155  of  2,031.    Elapsed: 0:09:09. Loss: 0.42058\n",
            "  Batch 1,160  of  2,031.    Elapsed: 0:09:12. Loss: 0.42060\n",
            "  Batch 1,165  of  2,031.    Elapsed: 0:09:14. Loss: 0.42059\n",
            "  Batch 1,170  of  2,031.    Elapsed: 0:09:16. Loss: 0.42061\n",
            "  Batch 1,175  of  2,031.    Elapsed: 0:09:19. Loss: 0.42048\n",
            "  Batch 1,180  of  2,031.    Elapsed: 0:09:21. Loss: 0.42068\n",
            "  Batch 1,185  of  2,031.    Elapsed: 0:09:24. Loss: 0.42052\n",
            "  Batch 1,190  of  2,031.    Elapsed: 0:09:26. Loss: 0.42057\n",
            "  Batch 1,195  of  2,031.    Elapsed: 0:09:28. Loss: 0.42058\n",
            "  Batch 1,200  of  2,031.    Elapsed: 0:09:31. Loss: 0.42008\n",
            "  Batch 1,205  of  2,031.    Elapsed: 0:09:33. Loss: 0.42014\n",
            "  Batch 1,210  of  2,031.    Elapsed: 0:09:35. Loss: 0.41992\n",
            "  Batch 1,215  of  2,031.    Elapsed: 0:09:38. Loss: 0.41985\n",
            "  Batch 1,220  of  2,031.    Elapsed: 0:09:40. Loss: 0.42002\n",
            "  Batch 1,225  of  2,031.    Elapsed: 0:09:43. Loss: 0.42022\n",
            "  Batch 1,230  of  2,031.    Elapsed: 0:09:45. Loss: 0.42014\n",
            "  Batch 1,235  of  2,031.    Elapsed: 0:09:47. Loss: 0.42018\n",
            "  Batch 1,240  of  2,031.    Elapsed: 0:09:50. Loss: 0.42005\n",
            "  Batch 1,245  of  2,031.    Elapsed: 0:09:52. Loss: 0.42006\n",
            "  Batch 1,250  of  2,031.    Elapsed: 0:09:55. Loss: 0.42015\n",
            "  Batch 1,255  of  2,031.    Elapsed: 0:09:57. Loss: 0.42035\n",
            "  Batch 1,260  of  2,031.    Elapsed: 0:09:59. Loss: 0.42015\n",
            "  Batch 1,265  of  2,031.    Elapsed: 0:10:02. Loss: 0.42012\n",
            "  Batch 1,270  of  2,031.    Elapsed: 0:10:04. Loss: 0.42008\n",
            "  Batch 1,275  of  2,031.    Elapsed: 0:10:06. Loss: 0.41991\n",
            "  Batch 1,280  of  2,031.    Elapsed: 0:10:09. Loss: 0.42002\n",
            "  Batch 1,285  of  2,031.    Elapsed: 0:10:11. Loss: 0.42001\n",
            "  Batch 1,290  of  2,031.    Elapsed: 0:10:14. Loss: 0.42025\n",
            "  Batch 1,295  of  2,031.    Elapsed: 0:10:16. Loss: 0.42036\n",
            "  Batch 1,300  of  2,031.    Elapsed: 0:10:18. Loss: 0.42041\n",
            "  Batch 1,305  of  2,031.    Elapsed: 0:10:21. Loss: 0.42051\n",
            "  Batch 1,310  of  2,031.    Elapsed: 0:10:23. Loss: 0.42039\n",
            "  Batch 1,315  of  2,031.    Elapsed: 0:10:25. Loss: 0.42024\n",
            "  Batch 1,320  of  2,031.    Elapsed: 0:10:28. Loss: 0.42039\n",
            "  Batch 1,325  of  2,031.    Elapsed: 0:10:30. Loss: 0.42051\n",
            "  Batch 1,330  of  2,031.    Elapsed: 0:10:33. Loss: 0.42066\n",
            "  Batch 1,335  of  2,031.    Elapsed: 0:10:35. Loss: 0.42057\n",
            "  Batch 1,340  of  2,031.    Elapsed: 0:10:37. Loss: 0.42060\n",
            "  Batch 1,345  of  2,031.    Elapsed: 0:10:40. Loss: 0.42078\n",
            "  Batch 1,350  of  2,031.    Elapsed: 0:10:42. Loss: 0.42082\n",
            "  Batch 1,355  of  2,031.    Elapsed: 0:10:44. Loss: 0.42073\n",
            "  Batch 1,360  of  2,031.    Elapsed: 0:10:47. Loss: 0.42065\n",
            "  Batch 1,365  of  2,031.    Elapsed: 0:10:49. Loss: 0.42064\n",
            "  Batch 1,370  of  2,031.    Elapsed: 0:10:52. Loss: 0.42058\n",
            "  Batch 1,375  of  2,031.    Elapsed: 0:10:54. Loss: 0.42063\n",
            "  Batch 1,380  of  2,031.    Elapsed: 0:10:56. Loss: 0.42054\n",
            "  Batch 1,385  of  2,031.    Elapsed: 0:10:59. Loss: 0.42029\n",
            "  Batch 1,390  of  2,031.    Elapsed: 0:11:01. Loss: 0.42030\n",
            "  Batch 1,395  of  2,031.    Elapsed: 0:11:03. Loss: 0.42010\n",
            "  Batch 1,400  of  2,031.    Elapsed: 0:11:06. Loss: 0.41996\n",
            "  Batch 1,405  of  2,031.    Elapsed: 0:11:08. Loss: 0.41994\n",
            "  Batch 1,410  of  2,031.    Elapsed: 0:11:11. Loss: 0.42005\n",
            "  Batch 1,415  of  2,031.    Elapsed: 0:11:13. Loss: 0.41986\n",
            "  Batch 1,420  of  2,031.    Elapsed: 0:11:15. Loss: 0.41974\n",
            "  Batch 1,425  of  2,031.    Elapsed: 0:11:18. Loss: 0.41945\n",
            "  Batch 1,430  of  2,031.    Elapsed: 0:11:20. Loss: 0.41926\n",
            "  Batch 1,435  of  2,031.    Elapsed: 0:11:22. Loss: 0.41940\n",
            "  Batch 1,440  of  2,031.    Elapsed: 0:11:25. Loss: 0.41950\n",
            "  Batch 1,445  of  2,031.    Elapsed: 0:11:27. Loss: 0.41960\n",
            "  Batch 1,450  of  2,031.    Elapsed: 0:11:30. Loss: 0.41974\n",
            "  Batch 1,455  of  2,031.    Elapsed: 0:11:32. Loss: 0.41983\n",
            "  Batch 1,460  of  2,031.    Elapsed: 0:11:34. Loss: 0.41990\n",
            "  Batch 1,465  of  2,031.    Elapsed: 0:11:37. Loss: 0.41971\n",
            "  Batch 1,470  of  2,031.    Elapsed: 0:11:39. Loss: 0.41982\n",
            "  Batch 1,475  of  2,031.    Elapsed: 0:11:41. Loss: 0.41989\n",
            "  Batch 1,480  of  2,031.    Elapsed: 0:11:44. Loss: 0.42010\n",
            "  Batch 1,485  of  2,031.    Elapsed: 0:11:46. Loss: 0.41995\n",
            "  Batch 1,490  of  2,031.    Elapsed: 0:11:49. Loss: 0.41973\n",
            "  Batch 1,495  of  2,031.    Elapsed: 0:11:51. Loss: 0.41987\n",
            "  Batch 1,500  of  2,031.    Elapsed: 0:11:53. Loss: 0.41965\n",
            "  Batch 1,505  of  2,031.    Elapsed: 0:11:56. Loss: 0.41950\n",
            "  Batch 1,510  of  2,031.    Elapsed: 0:11:58. Loss: 0.41962\n",
            "  Batch 1,515  of  2,031.    Elapsed: 0:12:01. Loss: 0.41942\n",
            "  Batch 1,520  of  2,031.    Elapsed: 0:12:03. Loss: 0.41960\n",
            "  Batch 1,525  of  2,031.    Elapsed: 0:12:05. Loss: 0.41971\n",
            "  Batch 1,530  of  2,031.    Elapsed: 0:12:08. Loss: 0.41956\n",
            "  Batch 1,535  of  2,031.    Elapsed: 0:12:10. Loss: 0.41944\n",
            "  Batch 1,540  of  2,031.    Elapsed: 0:12:12. Loss: 0.41942\n",
            "  Batch 1,545  of  2,031.    Elapsed: 0:12:15. Loss: 0.41962\n",
            "  Batch 1,550  of  2,031.    Elapsed: 0:12:17. Loss: 0.41969\n",
            "  Batch 1,555  of  2,031.    Elapsed: 0:12:20. Loss: 0.41943\n",
            "  Batch 1,560  of  2,031.    Elapsed: 0:12:22. Loss: 0.41935\n",
            "  Batch 1,565  of  2,031.    Elapsed: 0:12:24. Loss: 0.41943\n",
            "  Batch 1,570  of  2,031.    Elapsed: 0:12:27. Loss: 0.41949\n",
            "  Batch 1,575  of  2,031.    Elapsed: 0:12:29. Loss: 0.41924\n",
            "  Batch 1,580  of  2,031.    Elapsed: 0:12:31. Loss: 0.41933\n",
            "  Batch 1,585  of  2,031.    Elapsed: 0:12:34. Loss: 0.41931\n",
            "  Batch 1,590  of  2,031.    Elapsed: 0:12:36. Loss: 0.41922\n",
            "  Batch 1,595  of  2,031.    Elapsed: 0:12:38. Loss: 0.41925\n",
            "  Batch 1,600  of  2,031.    Elapsed: 0:12:41. Loss: 0.41928\n",
            "  Batch 1,605  of  2,031.    Elapsed: 0:12:43. Loss: 0.41925\n",
            "  Batch 1,610  of  2,031.    Elapsed: 0:12:46. Loss: 0.41910\n",
            "  Batch 1,615  of  2,031.    Elapsed: 0:12:48. Loss: 0.41905\n",
            "  Batch 1,620  of  2,031.    Elapsed: 0:12:50. Loss: 0.41886\n",
            "  Batch 1,625  of  2,031.    Elapsed: 0:12:53. Loss: 0.41907\n",
            "  Batch 1,630  of  2,031.    Elapsed: 0:12:55. Loss: 0.41914\n",
            "  Batch 1,635  of  2,031.    Elapsed: 0:12:57. Loss: 0.41912\n",
            "  Batch 1,640  of  2,031.    Elapsed: 0:13:00. Loss: 0.41929\n",
            "  Batch 1,645  of  2,031.    Elapsed: 0:13:02. Loss: 0.41928\n",
            "  Batch 1,650  of  2,031.    Elapsed: 0:13:05. Loss: 0.41921\n",
            "  Batch 1,655  of  2,031.    Elapsed: 0:13:07. Loss: 0.41920\n",
            "  Batch 1,660  of  2,031.    Elapsed: 0:13:09. Loss: 0.41942\n",
            "  Batch 1,665  of  2,031.    Elapsed: 0:13:12. Loss: 0.41938\n",
            "  Batch 1,670  of  2,031.    Elapsed: 0:13:14. Loss: 0.41934\n",
            "  Batch 1,675  of  2,031.    Elapsed: 0:13:17. Loss: 0.41927\n",
            "  Batch 1,680  of  2,031.    Elapsed: 0:13:19. Loss: 0.41908\n",
            "  Batch 1,685  of  2,031.    Elapsed: 0:13:21. Loss: 0.41913\n",
            "  Batch 1,690  of  2,031.    Elapsed: 0:13:24. Loss: 0.41938\n",
            "  Batch 1,695  of  2,031.    Elapsed: 0:13:26. Loss: 0.41950\n",
            "  Batch 1,700  of  2,031.    Elapsed: 0:13:28. Loss: 0.41928\n",
            "  Batch 1,705  of  2,031.    Elapsed: 0:13:31. Loss: 0.41922\n",
            "  Batch 1,710  of  2,031.    Elapsed: 0:13:33. Loss: 0.41946\n",
            "  Batch 1,715  of  2,031.    Elapsed: 0:13:36. Loss: 0.41967\n",
            "  Batch 1,720  of  2,031.    Elapsed: 0:13:38. Loss: 0.41977\n",
            "  Batch 1,725  of  2,031.    Elapsed: 0:13:40. Loss: 0.41974\n",
            "  Batch 1,730  of  2,031.    Elapsed: 0:13:43. Loss: 0.41968\n",
            "  Batch 1,735  of  2,031.    Elapsed: 0:13:45. Loss: 0.41972\n",
            "  Batch 1,740  of  2,031.    Elapsed: 0:13:47. Loss: 0.41976\n",
            "  Batch 1,745  of  2,031.    Elapsed: 0:13:50. Loss: 0.41978\n",
            "  Batch 1,750  of  2,031.    Elapsed: 0:13:52. Loss: 0.41980\n",
            "  Batch 1,755  of  2,031.    Elapsed: 0:13:55. Loss: 0.41970\n",
            "  Batch 1,760  of  2,031.    Elapsed: 0:13:57. Loss: 0.41961\n",
            "  Batch 1,765  of  2,031.    Elapsed: 0:13:59. Loss: 0.41954\n",
            "  Batch 1,770  of  2,031.    Elapsed: 0:14:02. Loss: 0.41941\n",
            "  Batch 1,775  of  2,031.    Elapsed: 0:14:04. Loss: 0.41934\n",
            "  Batch 1,780  of  2,031.    Elapsed: 0:14:06. Loss: 0.41927\n",
            "  Batch 1,785  of  2,031.    Elapsed: 0:14:09. Loss: 0.41948\n",
            "  Batch 1,790  of  2,031.    Elapsed: 0:14:11. Loss: 0.41950\n",
            "  Batch 1,795  of  2,031.    Elapsed: 0:14:14. Loss: 0.41947\n",
            "  Batch 1,800  of  2,031.    Elapsed: 0:14:16. Loss: 0.41951\n",
            "  Batch 1,805  of  2,031.    Elapsed: 0:14:18. Loss: 0.41952\n",
            "  Batch 1,810  of  2,031.    Elapsed: 0:14:21. Loss: 0.41970\n",
            "  Batch 1,815  of  2,031.    Elapsed: 0:14:23. Loss: 0.41969\n",
            "  Batch 1,820  of  2,031.    Elapsed: 0:14:25. Loss: 0.41977\n",
            "  Batch 1,825  of  2,031.    Elapsed: 0:14:28. Loss: 0.41974\n",
            "  Batch 1,830  of  2,031.    Elapsed: 0:14:30. Loss: 0.41978\n",
            "  Batch 1,835  of  2,031.    Elapsed: 0:14:33. Loss: 0.41970\n",
            "  Batch 1,840  of  2,031.    Elapsed: 0:14:35. Loss: 0.41973\n",
            "  Batch 1,845  of  2,031.    Elapsed: 0:14:37. Loss: 0.41980\n",
            "  Batch 1,850  of  2,031.    Elapsed: 0:14:40. Loss: 0.41986\n",
            "  Batch 1,855  of  2,031.    Elapsed: 0:14:42. Loss: 0.41989\n",
            "  Batch 1,860  of  2,031.    Elapsed: 0:14:44. Loss: 0.41989\n",
            "  Batch 1,865  of  2,031.    Elapsed: 0:14:47. Loss: 0.41980\n",
            "  Batch 1,870  of  2,031.    Elapsed: 0:14:49. Loss: 0.41985\n",
            "  Batch 1,875  of  2,031.    Elapsed: 0:14:52. Loss: 0.42011\n",
            "  Batch 1,880  of  2,031.    Elapsed: 0:14:54. Loss: 0.41999\n",
            "  Batch 1,885  of  2,031.    Elapsed: 0:14:56. Loss: 0.42015\n",
            "  Batch 1,890  of  2,031.    Elapsed: 0:14:59. Loss: 0.41998\n",
            "  Batch 1,895  of  2,031.    Elapsed: 0:15:01. Loss: 0.41998\n",
            "  Batch 1,900  of  2,031.    Elapsed: 0:15:03. Loss: 0.41996\n",
            "  Batch 1,905  of  2,031.    Elapsed: 0:15:06. Loss: 0.41990\n",
            "  Batch 1,910  of  2,031.    Elapsed: 0:15:08. Loss: 0.41987\n",
            "  Batch 1,915  of  2,031.    Elapsed: 0:15:11. Loss: 0.41989\n",
            "  Batch 1,920  of  2,031.    Elapsed: 0:15:13. Loss: 0.41995\n",
            "  Batch 1,925  of  2,031.    Elapsed: 0:15:15. Loss: 0.41985\n",
            "  Batch 1,930  of  2,031.    Elapsed: 0:15:18. Loss: 0.41988\n",
            "  Batch 1,935  of  2,031.    Elapsed: 0:15:20. Loss: 0.41985\n",
            "  Batch 1,940  of  2,031.    Elapsed: 0:15:22. Loss: 0.41973\n",
            "  Batch 1,945  of  2,031.    Elapsed: 0:15:25. Loss: 0.41985\n",
            "  Batch 1,950  of  2,031.    Elapsed: 0:15:27. Loss: 0.41968\n",
            "  Batch 1,955  of  2,031.    Elapsed: 0:15:30. Loss: 0.41981\n",
            "  Batch 1,960  of  2,031.    Elapsed: 0:15:32. Loss: 0.41980\n",
            "  Batch 1,965  of  2,031.    Elapsed: 0:15:34. Loss: 0.41985\n",
            "  Batch 1,970  of  2,031.    Elapsed: 0:15:37. Loss: 0.41981\n",
            "  Batch 1,975  of  2,031.    Elapsed: 0:15:39. Loss: 0.41987\n",
            "  Batch 1,980  of  2,031.    Elapsed: 0:15:41. Loss: 0.42009\n",
            "  Batch 1,985  of  2,031.    Elapsed: 0:15:44. Loss: 0.42016\n",
            "  Batch 1,990  of  2,031.    Elapsed: 0:15:46. Loss: 0.42025\n",
            "  Batch 1,995  of  2,031.    Elapsed: 0:15:49. Loss: 0.42031\n",
            "  Batch 2,000  of  2,031.    Elapsed: 0:15:51. Loss: 0.42038\n",
            "  Batch 2,005  of  2,031.    Elapsed: 0:15:53. Loss: 0.42048\n",
            "  Batch 2,010  of  2,031.    Elapsed: 0:15:56. Loss: 0.42059\n",
            "  Batch 2,015  of  2,031.    Elapsed: 0:15:58. Loss: 0.42062\n",
            "  Batch 2,020  of  2,031.    Elapsed: 0:16:00. Loss: 0.42058\n",
            "  Batch 2,025  of  2,031.    Elapsed: 0:16:03. Loss: 0.42055\n",
            "  Batch 2,030  of  2,031.    Elapsed: 0:16:05. Loss: 0.42067\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epcoh took: 0:16:05\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.40\n",
            "  Validation took: 0:01:10\n",
            "  Accuracy: 0.88\n",
            "  Accuracy: 0.92\n",
            "  Accuracy: 0.93\n",
            "  Accuracy: 0.85\n",
            "  Accuracy: 0.57\n",
            "  Macro F1-score: 0.47\n",
            "  Macro F1-score: 0.48\n",
            "  Macro F1-score: 0.48\n",
            "  Macro F1-score: 0.46\n",
            "  Macro F1-score: 0.37\n",
            "  Weighted F1-score: 0.82\n",
            "  Weighted F1-score: 0.89\n",
            "  Weighted F1-score: 0.89\n",
            "  Weighted F1-score: 0.78\n",
            "  Weighted F1-score: 0.42\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      1.00      0.94      3570\n",
            "         1.0       0.00      0.00      0.00       491\n",
            "\n",
            "    accuracy                           0.88      4061\n",
            "   macro avg       0.44      0.50      0.47      4061\n",
            "weighted avg       0.77      0.88      0.82      4061\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      1.00      0.96      3751\n",
            "         1.0       0.00      0.00      0.00       310\n",
            "\n",
            "    accuracy                           0.92      4061\n",
            "   macro avg       0.46      0.50      0.48      4061\n",
            "weighted avg       0.85      0.92      0.89      4061\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      1.00      0.96      3762\n",
            "         1.0       0.00      0.00      0.00       299\n",
            "\n",
            "    accuracy                           0.93      4061\n",
            "   macro avg       0.46      0.50      0.48      4061\n",
            "weighted avg       0.86      0.93      0.89      4061\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      1.00      0.92      3435\n",
            "         1.0       0.00      0.00      0.00       626\n",
            "\n",
            "    accuracy                           0.85      4061\n",
            "   macro avg       0.42      0.50      0.46      4061\n",
            "weighted avg       0.72      0.85      0.78      4061\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00      1726\n",
            "         1.0       0.57      1.00      0.73      2335\n",
            "\n",
            "    accuracy                           0.57      4061\n",
            "   macro avg       0.29      0.50      0.37      4061\n",
            "weighted avg       0.33      0.57      0.42      4061\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3570    0]\n",
            " [ 491    0]]\n",
            "[[3751    0]\n",
            " [ 310    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch     5  of  2,031.    Elapsed: 0:00:02. Loss: 0.45774\n",
            "  Batch    10  of  2,031.    Elapsed: 0:00:05. Loss: 0.43326\n",
            "  Batch    15  of  2,031.    Elapsed: 0:00:07. Loss: 0.42653\n",
            "  Batch    20  of  2,031.    Elapsed: 0:00:09. Loss: 0.43345\n",
            "  Batch    25  of  2,031.    Elapsed: 0:00:12. Loss: 0.42906\n",
            "  Batch    30  of  2,031.    Elapsed: 0:00:14. Loss: 0.41938\n",
            "  Batch    35  of  2,031.    Elapsed: 0:00:17. Loss: 0.41584\n",
            "  Batch    40  of  2,031.    Elapsed: 0:00:19. Loss: 0.41682\n",
            "  Batch    45  of  2,031.    Elapsed: 0:00:21. Loss: 0.42271\n",
            "  Batch    50  of  2,031.    Elapsed: 0:00:24. Loss: 0.41835\n",
            "  Batch    55  of  2,031.    Elapsed: 0:00:26. Loss: 0.42198\n",
            "  Batch    60  of  2,031.    Elapsed: 0:00:29. Loss: 0.42502\n",
            "  Batch    65  of  2,031.    Elapsed: 0:00:31. Loss: 0.42465\n",
            "  Batch    70  of  2,031.    Elapsed: 0:00:33. Loss: 0.42784\n",
            "  Batch    75  of  2,031.    Elapsed: 0:00:36. Loss: 0.43087\n",
            "  Batch    80  of  2,031.    Elapsed: 0:00:38. Loss: 0.42523\n",
            "  Batch    85  of  2,031.    Elapsed: 0:00:41. Loss: 0.42397\n",
            "  Batch    90  of  2,031.    Elapsed: 0:00:43. Loss: 0.42590\n",
            "  Batch    95  of  2,031.    Elapsed: 0:00:45. Loss: 0.42881\n",
            "  Batch   100  of  2,031.    Elapsed: 0:00:48. Loss: 0.42628\n",
            "  Batch   105  of  2,031.    Elapsed: 0:00:50. Loss: 0.42427\n",
            "  Batch   110  of  2,031.    Elapsed: 0:00:53. Loss: 0.42378\n",
            "  Batch   115  of  2,031.    Elapsed: 0:00:55. Loss: 0.42194\n",
            "  Batch   120  of  2,031.    Elapsed: 0:00:57. Loss: 0.41985\n",
            "  Batch   125  of  2,031.    Elapsed: 0:01:00. Loss: 0.41730\n",
            "  Batch   130  of  2,031.    Elapsed: 0:01:02. Loss: 0.41740\n",
            "  Batch   135  of  2,031.    Elapsed: 0:01:04. Loss: 0.41879\n",
            "  Batch   140  of  2,031.    Elapsed: 0:01:07. Loss: 0.41837\n",
            "  Batch   145  of  2,031.    Elapsed: 0:01:09. Loss: 0.41703\n",
            "  Batch   150  of  2,031.    Elapsed: 0:01:12. Loss: 0.41699\n",
            "  Batch   155  of  2,031.    Elapsed: 0:01:14. Loss: 0.41866\n",
            "  Batch   160  of  2,031.    Elapsed: 0:01:16. Loss: 0.42016\n",
            "  Batch   165  of  2,031.    Elapsed: 0:01:19. Loss: 0.42049\n",
            "  Batch   170  of  2,031.    Elapsed: 0:01:21. Loss: 0.41938\n",
            "  Batch   175  of  2,031.    Elapsed: 0:01:23. Loss: 0.42015\n",
            "  Batch   180  of  2,031.    Elapsed: 0:01:26. Loss: 0.42212\n",
            "  Batch   185  of  2,031.    Elapsed: 0:01:28. Loss: 0.42192\n",
            "  Batch   190  of  2,031.    Elapsed: 0:01:31. Loss: 0.42073\n",
            "  Batch   195  of  2,031.    Elapsed: 0:01:33. Loss: 0.42043\n",
            "  Batch   200  of  2,031.    Elapsed: 0:01:35. Loss: 0.42179\n",
            "  Batch   205  of  2,031.    Elapsed: 0:01:38. Loss: 0.42139\n",
            "  Batch   210  of  2,031.    Elapsed: 0:01:40. Loss: 0.42150\n",
            "  Batch   215  of  2,031.    Elapsed: 0:01:42. Loss: 0.41972\n",
            "  Batch   220  of  2,031.    Elapsed: 0:01:45. Loss: 0.41857\n",
            "  Batch   225  of  2,031.    Elapsed: 0:01:47. Loss: 0.41898\n",
            "  Batch   230  of  2,031.    Elapsed: 0:01:49. Loss: 0.41865\n",
            "  Batch   235  of  2,031.    Elapsed: 0:01:52. Loss: 0.41799\n",
            "  Batch   240  of  2,031.    Elapsed: 0:01:54. Loss: 0.42010\n",
            "  Batch   245  of  2,031.    Elapsed: 0:01:57. Loss: 0.41992\n",
            "  Batch   250  of  2,031.    Elapsed: 0:01:59. Loss: 0.41956\n",
            "  Batch   255  of  2,031.    Elapsed: 0:02:01. Loss: 0.41927\n",
            "  Batch   260  of  2,031.    Elapsed: 0:02:04. Loss: 0.41929\n",
            "  Batch   265  of  2,031.    Elapsed: 0:02:06. Loss: 0.41955\n",
            "  Batch   270  of  2,031.    Elapsed: 0:02:09. Loss: 0.42101\n",
            "  Batch   275  of  2,031.    Elapsed: 0:02:11. Loss: 0.41949\n",
            "  Batch   280  of  2,031.    Elapsed: 0:02:13. Loss: 0.41938\n",
            "  Batch   285  of  2,031.    Elapsed: 0:02:16. Loss: 0.42025\n",
            "  Batch   290  of  2,031.    Elapsed: 0:02:18. Loss: 0.42057\n",
            "  Batch   295  of  2,031.    Elapsed: 0:02:20. Loss: 0.42075\n",
            "  Batch   300  of  2,031.    Elapsed: 0:02:23. Loss: 0.42078\n",
            "  Batch   305  of  2,031.    Elapsed: 0:02:25. Loss: 0.42072\n",
            "  Batch   310  of  2,031.    Elapsed: 0:02:28. Loss: 0.42188\n",
            "  Batch   315  of  2,031.    Elapsed: 0:02:30. Loss: 0.42228\n",
            "  Batch   320  of  2,031.    Elapsed: 0:02:32. Loss: 0.42222\n",
            "  Batch   325  of  2,031.    Elapsed: 0:02:35. Loss: 0.42232\n",
            "  Batch   330  of  2,031.    Elapsed: 0:02:37. Loss: 0.42179\n",
            "  Batch   335  of  2,031.    Elapsed: 0:02:39. Loss: 0.42053\n",
            "  Batch   340  of  2,031.    Elapsed: 0:02:42. Loss: 0.42077\n",
            "  Batch   345  of  2,031.    Elapsed: 0:02:44. Loss: 0.42115\n",
            "  Batch   350  of  2,031.    Elapsed: 0:02:47. Loss: 0.42172\n",
            "  Batch   355  of  2,031.    Elapsed: 0:02:49. Loss: 0.42110\n",
            "  Batch   360  of  2,031.    Elapsed: 0:02:51. Loss: 0.42065\n",
            "  Batch   365  of  2,031.    Elapsed: 0:02:54. Loss: 0.42105\n",
            "  Batch   370  of  2,031.    Elapsed: 0:02:56. Loss: 0.42154\n",
            "  Batch   375  of  2,031.    Elapsed: 0:02:59. Loss: 0.42206\n",
            "  Batch   380  of  2,031.    Elapsed: 0:03:01. Loss: 0.42178\n",
            "  Batch   385  of  2,031.    Elapsed: 0:03:03. Loss: 0.42222\n",
            "  Batch   390  of  2,031.    Elapsed: 0:03:06. Loss: 0.42186\n",
            "  Batch   395  of  2,031.    Elapsed: 0:03:08. Loss: 0.42206\n",
            "  Batch   400  of  2,031.    Elapsed: 0:03:10. Loss: 0.42255\n",
            "  Batch   405  of  2,031.    Elapsed: 0:03:13. Loss: 0.42275\n",
            "  Batch   410  of  2,031.    Elapsed: 0:03:15. Loss: 0.42328\n",
            "  Batch   415  of  2,031.    Elapsed: 0:03:18. Loss: 0.42413\n",
            "  Batch   420  of  2,031.    Elapsed: 0:03:20. Loss: 0.42413\n",
            "  Batch   425  of  2,031.    Elapsed: 0:03:22. Loss: 0.42457\n",
            "  Batch   430  of  2,031.    Elapsed: 0:03:25. Loss: 0.42502\n",
            "  Batch   435  of  2,031.    Elapsed: 0:03:27. Loss: 0.42457\n",
            "  Batch   440  of  2,031.    Elapsed: 0:03:29. Loss: 0.42486\n",
            "  Batch   445  of  2,031.    Elapsed: 0:03:32. Loss: 0.42442\n",
            "  Batch   450  of  2,031.    Elapsed: 0:03:34. Loss: 0.42422\n",
            "  Batch   455  of  2,031.    Elapsed: 0:03:37. Loss: 0.42387\n",
            "  Batch   460  of  2,031.    Elapsed: 0:03:39. Loss: 0.42439\n",
            "  Batch   465  of  2,031.    Elapsed: 0:03:41. Loss: 0.42398\n",
            "  Batch   470  of  2,031.    Elapsed: 0:03:44. Loss: 0.42342\n",
            "  Batch   475  of  2,031.    Elapsed: 0:03:46. Loss: 0.42398\n",
            "  Batch   480  of  2,031.    Elapsed: 0:03:48. Loss: 0.42344\n",
            "  Batch   485  of  2,031.    Elapsed: 0:03:51. Loss: 0.42337\n",
            "  Batch   490  of  2,031.    Elapsed: 0:03:53. Loss: 0.42384\n",
            "  Batch   495  of  2,031.    Elapsed: 0:03:56. Loss: 0.42381\n",
            "  Batch   500  of  2,031.    Elapsed: 0:03:58. Loss: 0.42389\n",
            "  Batch   505  of  2,031.    Elapsed: 0:04:00. Loss: 0.42377\n",
            "  Batch   510  of  2,031.    Elapsed: 0:04:03. Loss: 0.42327\n",
            "  Batch   515  of  2,031.    Elapsed: 0:04:05. Loss: 0.42378\n",
            "  Batch   520  of  2,031.    Elapsed: 0:04:07. Loss: 0.42343\n",
            "  Batch   525  of  2,031.    Elapsed: 0:04:10. Loss: 0.42341\n",
            "  Batch   530  of  2,031.    Elapsed: 0:04:12. Loss: 0.42312\n",
            "  Batch   535  of  2,031.    Elapsed: 0:04:15. Loss: 0.42306\n",
            "  Batch   540  of  2,031.    Elapsed: 0:04:17. Loss: 0.42303\n",
            "  Batch   545  of  2,031.    Elapsed: 0:04:19. Loss: 0.42318\n",
            "  Batch   550  of  2,031.    Elapsed: 0:04:22. Loss: 0.42327\n",
            "  Batch   555  of  2,031.    Elapsed: 0:04:24. Loss: 0.42351\n",
            "  Batch   560  of  2,031.    Elapsed: 0:04:26. Loss: 0.42292\n",
            "  Batch   565  of  2,031.    Elapsed: 0:04:29. Loss: 0.42294\n",
            "  Batch   570  of  2,031.    Elapsed: 0:04:31. Loss: 0.42324\n",
            "  Batch   575  of  2,031.    Elapsed: 0:04:34. Loss: 0.42288\n",
            "  Batch   580  of  2,031.    Elapsed: 0:04:36. Loss: 0.42328\n",
            "  Batch   585  of  2,031.    Elapsed: 0:04:38. Loss: 0.42339\n",
            "  Batch   590  of  2,031.    Elapsed: 0:04:41. Loss: 0.42342\n",
            "  Batch   595  of  2,031.    Elapsed: 0:04:43. Loss: 0.42302\n",
            "  Batch   600  of  2,031.    Elapsed: 0:04:45. Loss: 0.42306\n",
            "  Batch   605  of  2,031.    Elapsed: 0:04:48. Loss: 0.42300\n",
            "  Batch   610  of  2,031.    Elapsed: 0:04:50. Loss: 0.42239\n",
            "  Batch   615  of  2,031.    Elapsed: 0:04:53. Loss: 0.42248\n",
            "  Batch   620  of  2,031.    Elapsed: 0:04:55. Loss: 0.42211\n",
            "  Batch   625  of  2,031.    Elapsed: 0:04:57. Loss: 0.42164\n",
            "  Batch   630  of  2,031.    Elapsed: 0:05:00. Loss: 0.42143\n",
            "  Batch   635  of  2,031.    Elapsed: 0:05:02. Loss: 0.42207\n",
            "  Batch   640  of  2,031.    Elapsed: 0:05:05. Loss: 0.42127\n",
            "  Batch   645  of  2,031.    Elapsed: 0:05:07. Loss: 0.42128\n",
            "  Batch   650  of  2,031.    Elapsed: 0:05:09. Loss: 0.42079\n",
            "  Batch   655  of  2,031.    Elapsed: 0:05:12. Loss: 0.42051\n",
            "  Batch   660  of  2,031.    Elapsed: 0:05:14. Loss: 0.42020\n",
            "  Batch   665  of  2,031.    Elapsed: 0:05:16. Loss: 0.41977\n",
            "  Batch   670  of  2,031.    Elapsed: 0:05:19. Loss: 0.41971\n",
            "  Batch   675  of  2,031.    Elapsed: 0:05:21. Loss: 0.41947\n",
            "  Batch   680  of  2,031.    Elapsed: 0:05:24. Loss: 0.41915\n",
            "  Batch   685  of  2,031.    Elapsed: 0:05:26. Loss: 0.41935\n",
            "  Batch   690  of  2,031.    Elapsed: 0:05:28. Loss: 0.41968\n",
            "  Batch   695  of  2,031.    Elapsed: 0:05:31. Loss: 0.41971\n",
            "  Batch   700  of  2,031.    Elapsed: 0:05:33. Loss: 0.41935\n",
            "  Batch   705  of  2,031.    Elapsed: 0:05:35. Loss: 0.41949\n",
            "  Batch   710  of  2,031.    Elapsed: 0:05:38. Loss: 0.41939\n",
            "  Batch   715  of  2,031.    Elapsed: 0:05:40. Loss: 0.41942\n",
            "  Batch   720  of  2,031.    Elapsed: 0:05:43. Loss: 0.41932\n",
            "  Batch   725  of  2,031.    Elapsed: 0:05:45. Loss: 0.41916\n",
            "  Batch   730  of  2,031.    Elapsed: 0:05:47. Loss: 0.41942\n",
            "  Batch   735  of  2,031.    Elapsed: 0:05:50. Loss: 0.41991\n",
            "  Batch   740  of  2,031.    Elapsed: 0:05:52. Loss: 0.41991\n",
            "  Batch   745  of  2,031.    Elapsed: 0:05:54. Loss: 0.41993\n",
            "  Batch   750  of  2,031.    Elapsed: 0:05:57. Loss: 0.42052\n",
            "  Batch   755  of  2,031.    Elapsed: 0:05:59. Loss: 0.42094\n",
            "  Batch   760  of  2,031.    Elapsed: 0:06:02. Loss: 0.42083\n",
            "  Batch   765  of  2,031.    Elapsed: 0:06:04. Loss: 0.42052\n",
            "  Batch   770  of  2,031.    Elapsed: 0:06:06. Loss: 0.42063\n",
            "  Batch   775  of  2,031.    Elapsed: 0:06:09. Loss: 0.42091\n",
            "  Batch   780  of  2,031.    Elapsed: 0:06:11. Loss: 0.42093\n",
            "  Batch   785  of  2,031.    Elapsed: 0:06:13. Loss: 0.42086\n",
            "  Batch   790  of  2,031.    Elapsed: 0:06:16. Loss: 0.42107\n",
            "  Batch   795  of  2,031.    Elapsed: 0:06:18. Loss: 0.42100\n",
            "  Batch   800  of  2,031.    Elapsed: 0:06:21. Loss: 0.42112\n",
            "  Batch   805  of  2,031.    Elapsed: 0:06:23. Loss: 0.42133\n",
            "  Batch   810  of  2,031.    Elapsed: 0:06:26. Loss: 0.42099\n",
            "  Batch   815  of  2,031.    Elapsed: 0:06:28. Loss: 0.42069\n",
            "  Batch   820  of  2,031.    Elapsed: 0:06:30. Loss: 0.42099\n",
            "  Batch   825  of  2,031.    Elapsed: 0:06:33. Loss: 0.42096\n",
            "  Batch   830  of  2,031.    Elapsed: 0:06:35. Loss: 0.42098\n",
            "  Batch   835  of  2,031.    Elapsed: 0:06:37. Loss: 0.42060\n",
            "  Batch   840  of  2,031.    Elapsed: 0:06:40. Loss: 0.42033\n",
            "  Batch   845  of  2,031.    Elapsed: 0:06:42. Loss: 0.41988\n",
            "  Batch   850  of  2,031.    Elapsed: 0:06:45. Loss: 0.41956\n",
            "  Batch   855  of  2,031.    Elapsed: 0:06:47. Loss: 0.41947\n",
            "  Batch   860  of  2,031.    Elapsed: 0:06:49. Loss: 0.41984\n",
            "  Batch   865  of  2,031.    Elapsed: 0:06:52. Loss: 0.41972\n",
            "  Batch   870  of  2,031.    Elapsed: 0:06:54. Loss: 0.41964\n",
            "  Batch   875  of  2,031.    Elapsed: 0:06:56. Loss: 0.41949\n",
            "  Batch   880  of  2,031.    Elapsed: 0:06:59. Loss: 0.41955\n",
            "  Batch   885  of  2,031.    Elapsed: 0:07:01. Loss: 0.41958\n",
            "  Batch   890  of  2,031.    Elapsed: 0:07:04. Loss: 0.41922\n",
            "  Batch   895  of  2,031.    Elapsed: 0:07:06. Loss: 0.41963\n",
            "  Batch   900  of  2,031.    Elapsed: 0:07:08. Loss: 0.42002\n",
            "  Batch   905  of  2,031.    Elapsed: 0:07:11. Loss: 0.41993\n",
            "  Batch   910  of  2,031.    Elapsed: 0:07:13. Loss: 0.42009\n",
            "  Batch   915  of  2,031.    Elapsed: 0:07:15. Loss: 0.41993\n",
            "  Batch   920  of  2,031.    Elapsed: 0:07:18. Loss: 0.41980\n",
            "  Batch   925  of  2,031.    Elapsed: 0:07:20. Loss: 0.41967\n",
            "  Batch   930  of  2,031.    Elapsed: 0:07:23. Loss: 0.42020\n",
            "  Batch   935  of  2,031.    Elapsed: 0:07:25. Loss: 0.42018\n",
            "  Batch   940  of  2,031.    Elapsed: 0:07:27. Loss: 0.42024\n",
            "  Batch   945  of  2,031.    Elapsed: 0:07:30. Loss: 0.42055\n",
            "  Batch   950  of  2,031.    Elapsed: 0:07:32. Loss: 0.42046\n",
            "  Batch   955  of  2,031.    Elapsed: 0:07:34. Loss: 0.42035\n",
            "  Batch   960  of  2,031.    Elapsed: 0:07:37. Loss: 0.42050\n",
            "  Batch   965  of  2,031.    Elapsed: 0:07:39. Loss: 0.42050\n",
            "  Batch   970  of  2,031.    Elapsed: 0:07:42. Loss: 0.42091\n",
            "  Batch   975  of  2,031.    Elapsed: 0:07:44. Loss: 0.42087\n",
            "  Batch   980  of  2,031.    Elapsed: 0:07:46. Loss: 0.42073\n",
            "  Batch   985  of  2,031.    Elapsed: 0:07:49. Loss: 0.42055\n",
            "  Batch   990  of  2,031.    Elapsed: 0:07:51. Loss: 0.42043\n",
            "  Batch   995  of  2,031.    Elapsed: 0:07:53. Loss: 0.42057\n",
            "  Batch 1,000  of  2,031.    Elapsed: 0:07:56. Loss: 0.42039\n",
            "  Batch 1,005  of  2,031.    Elapsed: 0:07:58. Loss: 0.42048\n",
            "  Batch 1,010  of  2,031.    Elapsed: 0:08:01. Loss: 0.42033\n",
            "  Batch 1,015  of  2,031.    Elapsed: 0:08:03. Loss: 0.42023\n",
            "  Batch 1,020  of  2,031.    Elapsed: 0:08:05. Loss: 0.42021\n",
            "  Batch 1,025  of  2,031.    Elapsed: 0:08:08. Loss: 0.41990\n",
            "  Batch 1,030  of  2,031.    Elapsed: 0:08:10. Loss: 0.42009\n",
            "  Batch 1,035  of  2,031.    Elapsed: 0:08:12. Loss: 0.41980\n",
            "  Batch 1,040  of  2,031.    Elapsed: 0:08:15. Loss: 0.41983\n",
            "  Batch 1,045  of  2,031.    Elapsed: 0:08:17. Loss: 0.41959\n",
            "  Batch 1,050  of  2,031.    Elapsed: 0:08:20. Loss: 0.41954\n",
            "  Batch 1,055  of  2,031.    Elapsed: 0:08:22. Loss: 0.41915\n",
            "  Batch 1,060  of  2,031.    Elapsed: 0:08:24. Loss: 0.41929\n",
            "  Batch 1,065  of  2,031.    Elapsed: 0:08:27. Loss: 0.41935\n",
            "  Batch 1,070  of  2,031.    Elapsed: 0:08:29. Loss: 0.41908\n",
            "  Batch 1,075  of  2,031.    Elapsed: 0:08:31. Loss: 0.41888\n",
            "  Batch 1,080  of  2,031.    Elapsed: 0:08:34. Loss: 0.41898\n",
            "  Batch 1,085  of  2,031.    Elapsed: 0:08:36. Loss: 0.41903\n",
            "  Batch 1,090  of  2,031.    Elapsed: 0:08:39. Loss: 0.41879\n",
            "  Batch 1,095  of  2,031.    Elapsed: 0:08:41. Loss: 0.41856\n",
            "  Batch 1,100  of  2,031.    Elapsed: 0:08:43. Loss: 0.41858\n",
            "  Batch 1,105  of  2,031.    Elapsed: 0:08:46. Loss: 0.41817\n",
            "  Batch 1,110  of  2,031.    Elapsed: 0:08:48. Loss: 0.41829\n",
            "  Batch 1,115  of  2,031.    Elapsed: 0:08:50. Loss: 0.41818\n",
            "  Batch 1,120  of  2,031.    Elapsed: 0:08:53. Loss: 0.41868\n",
            "  Batch 1,125  of  2,031.    Elapsed: 0:08:55. Loss: 0.41878\n",
            "  Batch 1,130  of  2,031.    Elapsed: 0:08:58. Loss: 0.41851\n",
            "  Batch 1,135  of  2,031.    Elapsed: 0:09:00. Loss: 0.41845\n",
            "  Batch 1,140  of  2,031.    Elapsed: 0:09:02. Loss: 0.41850\n",
            "  Batch 1,145  of  2,031.    Elapsed: 0:09:05. Loss: 0.41825\n",
            "  Batch 1,150  of  2,031.    Elapsed: 0:09:07. Loss: 0.41821\n",
            "  Batch 1,155  of  2,031.    Elapsed: 0:09:09. Loss: 0.41827\n",
            "  Batch 1,160  of  2,031.    Elapsed: 0:09:12. Loss: 0.41797\n",
            "  Batch 1,165  of  2,031.    Elapsed: 0:09:14. Loss: 0.41779\n",
            "  Batch 1,170  of  2,031.    Elapsed: 0:09:17. Loss: 0.41762\n",
            "  Batch 1,175  of  2,031.    Elapsed: 0:09:19. Loss: 0.41812\n",
            "  Batch 1,180  of  2,031.    Elapsed: 0:09:21. Loss: 0.41810\n",
            "  Batch 1,185  of  2,031.    Elapsed: 0:09:24. Loss: 0.41829\n",
            "  Batch 1,190  of  2,031.    Elapsed: 0:09:26. Loss: 0.41847\n",
            "  Batch 1,195  of  2,031.    Elapsed: 0:09:28. Loss: 0.41849\n",
            "  Batch 1,200  of  2,031.    Elapsed: 0:09:31. Loss: 0.41854\n",
            "  Batch 1,205  of  2,031.    Elapsed: 0:09:33. Loss: 0.41850\n",
            "  Batch 1,210  of  2,031.    Elapsed: 0:09:36. Loss: 0.41893\n",
            "  Batch 1,215  of  2,031.    Elapsed: 0:09:38. Loss: 0.41920\n",
            "  Batch 1,220  of  2,031.    Elapsed: 0:09:40. Loss: 0.41905\n",
            "  Batch 1,225  of  2,031.    Elapsed: 0:09:43. Loss: 0.41933\n",
            "  Batch 1,230  of  2,031.    Elapsed: 0:09:45. Loss: 0.41947\n",
            "  Batch 1,235  of  2,031.    Elapsed: 0:09:47. Loss: 0.41955\n",
            "  Batch 1,240  of  2,031.    Elapsed: 0:09:50. Loss: 0.41975\n",
            "  Batch 1,245  of  2,031.    Elapsed: 0:09:52. Loss: 0.41961\n",
            "  Batch 1,250  of  2,031.    Elapsed: 0:09:55. Loss: 0.41944\n",
            "  Batch 1,255  of  2,031.    Elapsed: 0:09:57. Loss: 0.41965\n",
            "  Batch 1,260  of  2,031.    Elapsed: 0:09:59. Loss: 0.41969\n",
            "  Batch 1,265  of  2,031.    Elapsed: 0:10:02. Loss: 0.42013\n",
            "  Batch 1,270  of  2,031.    Elapsed: 0:10:04. Loss: 0.42029\n",
            "  Batch 1,275  of  2,031.    Elapsed: 0:10:07. Loss: 0.42035\n",
            "  Batch 1,280  of  2,031.    Elapsed: 0:10:09. Loss: 0.42019\n",
            "  Batch 1,285  of  2,031.    Elapsed: 0:10:11. Loss: 0.42011\n",
            "  Batch 1,290  of  2,031.    Elapsed: 0:10:14. Loss: 0.42021\n",
            "  Batch 1,295  of  2,031.    Elapsed: 0:10:16. Loss: 0.42050\n",
            "  Batch 1,300  of  2,031.    Elapsed: 0:10:18. Loss: 0.42040\n",
            "  Batch 1,305  of  2,031.    Elapsed: 0:10:21. Loss: 0.42037\n",
            "  Batch 1,310  of  2,031.    Elapsed: 0:10:23. Loss: 0.42038\n",
            "  Batch 1,315  of  2,031.    Elapsed: 0:10:26. Loss: 0.42010\n",
            "  Batch 1,320  of  2,031.    Elapsed: 0:10:28. Loss: 0.42009\n",
            "  Batch 1,325  of  2,031.    Elapsed: 0:10:30. Loss: 0.42016\n",
            "  Batch 1,330  of  2,031.    Elapsed: 0:10:33. Loss: 0.42009\n",
            "  Batch 1,335  of  2,031.    Elapsed: 0:10:35. Loss: 0.41985\n",
            "  Batch 1,340  of  2,031.    Elapsed: 0:10:37. Loss: 0.41989\n",
            "  Batch 1,345  of  2,031.    Elapsed: 0:10:40. Loss: 0.41989\n",
            "  Batch 1,350  of  2,031.    Elapsed: 0:10:42. Loss: 0.42001\n",
            "  Batch 1,355  of  2,031.    Elapsed: 0:10:45. Loss: 0.42025\n",
            "  Batch 1,360  of  2,031.    Elapsed: 0:10:47. Loss: 0.42006\n",
            "  Batch 1,365  of  2,031.    Elapsed: 0:10:49. Loss: 0.42019\n",
            "  Batch 1,370  of  2,031.    Elapsed: 0:10:52. Loss: 0.42014\n",
            "  Batch 1,375  of  2,031.    Elapsed: 0:10:54. Loss: 0.42014\n",
            "  Batch 1,380  of  2,031.    Elapsed: 0:10:56. Loss: 0.42022\n",
            "  Batch 1,385  of  2,031.    Elapsed: 0:10:59. Loss: 0.42030\n",
            "  Batch 1,390  of  2,031.    Elapsed: 0:11:01. Loss: 0.42034\n",
            "  Batch 1,395  of  2,031.    Elapsed: 0:11:04. Loss: 0.42044\n",
            "  Batch 1,400  of  2,031.    Elapsed: 0:11:06. Loss: 0.42038\n",
            "  Batch 1,405  of  2,031.    Elapsed: 0:11:08. Loss: 0.42040\n",
            "  Batch 1,410  of  2,031.    Elapsed: 0:11:11. Loss: 0.42032\n",
            "  Batch 1,415  of  2,031.    Elapsed: 0:11:13. Loss: 0.42020\n",
            "  Batch 1,420  of  2,031.    Elapsed: 0:11:15. Loss: 0.42050\n",
            "  Batch 1,425  of  2,031.    Elapsed: 0:11:18. Loss: 0.42030\n",
            "  Batch 1,430  of  2,031.    Elapsed: 0:11:20. Loss: 0.42039\n",
            "  Batch 1,435  of  2,031.    Elapsed: 0:11:23. Loss: 0.42047\n",
            "  Batch 1,440  of  2,031.    Elapsed: 0:11:25. Loss: 0.42057\n",
            "  Batch 1,445  of  2,031.    Elapsed: 0:11:27. Loss: 0.42051\n",
            "  Batch 1,450  of  2,031.    Elapsed: 0:11:30. Loss: 0.42050\n",
            "  Batch 1,455  of  2,031.    Elapsed: 0:11:32. Loss: 0.42049\n",
            "  Batch 1,460  of  2,031.    Elapsed: 0:11:35. Loss: 0.42046\n",
            "  Batch 1,465  of  2,031.    Elapsed: 0:11:37. Loss: 0.42062\n",
            "  Batch 1,470  of  2,031.    Elapsed: 0:11:39. Loss: 0.42046\n",
            "  Batch 1,475  of  2,031.    Elapsed: 0:11:42. Loss: 0.42066\n",
            "  Batch 1,480  of  2,031.    Elapsed: 0:11:44. Loss: 0.42064\n",
            "  Batch 1,485  of  2,031.    Elapsed: 0:11:46. Loss: 0.42066\n",
            "  Batch 1,490  of  2,031.    Elapsed: 0:11:49. Loss: 0.42051\n",
            "  Batch 1,495  of  2,031.    Elapsed: 0:11:51. Loss: 0.42069\n",
            "  Batch 1,500  of  2,031.    Elapsed: 0:11:54. Loss: 0.42061\n",
            "  Batch 1,505  of  2,031.    Elapsed: 0:11:56. Loss: 0.42079\n",
            "  Batch 1,510  of  2,031.    Elapsed: 0:11:58. Loss: 0.42072\n",
            "  Batch 1,515  of  2,031.    Elapsed: 0:12:01. Loss: 0.42076\n",
            "  Batch 1,520  of  2,031.    Elapsed: 0:12:03. Loss: 0.42087\n",
            "  Batch 1,525  of  2,031.    Elapsed: 0:12:05. Loss: 0.42085\n",
            "  Batch 1,530  of  2,031.    Elapsed: 0:12:08. Loss: 0.42093\n",
            "  Batch 1,535  of  2,031.    Elapsed: 0:12:10. Loss: 0.42109\n",
            "  Batch 1,540  of  2,031.    Elapsed: 0:12:13. Loss: 0.42104\n",
            "  Batch 1,545  of  2,031.    Elapsed: 0:12:15. Loss: 0.42107\n",
            "  Batch 1,550  of  2,031.    Elapsed: 0:12:17. Loss: 0.42114\n",
            "  Batch 1,555  of  2,031.    Elapsed: 0:12:20. Loss: 0.42130\n",
            "  Batch 1,560  of  2,031.    Elapsed: 0:12:22. Loss: 0.42126\n",
            "  Batch 1,565  of  2,031.    Elapsed: 0:12:24. Loss: 0.42132\n",
            "  Batch 1,570  of  2,031.    Elapsed: 0:12:27. Loss: 0.42147\n",
            "  Batch 1,575  of  2,031.    Elapsed: 0:12:29. Loss: 0.42141\n",
            "  Batch 1,580  of  2,031.    Elapsed: 0:12:32. Loss: 0.42150\n",
            "  Batch 1,585  of  2,031.    Elapsed: 0:12:34. Loss: 0.42155\n",
            "  Batch 1,590  of  2,031.    Elapsed: 0:12:36. Loss: 0.42156\n",
            "  Batch 1,595  of  2,031.    Elapsed: 0:12:39. Loss: 0.42186\n",
            "  Batch 1,600  of  2,031.    Elapsed: 0:12:41. Loss: 0.42186\n",
            "  Batch 1,605  of  2,031.    Elapsed: 0:12:43. Loss: 0.42193\n",
            "  Batch 1,610  of  2,031.    Elapsed: 0:12:46. Loss: 0.42189\n",
            "  Batch 1,615  of  2,031.    Elapsed: 0:12:48. Loss: 0.42191\n",
            "  Batch 1,620  of  2,031.    Elapsed: 0:12:51. Loss: 0.42199\n",
            "  Batch 1,625  of  2,031.    Elapsed: 0:12:53. Loss: 0.42194\n",
            "  Batch 1,630  of  2,031.    Elapsed: 0:12:55. Loss: 0.42214\n",
            "  Batch 1,635  of  2,031.    Elapsed: 0:12:58. Loss: 0.42223\n",
            "  Batch 1,640  of  2,031.    Elapsed: 0:13:00. Loss: 0.42235\n",
            "  Batch 1,645  of  2,031.    Elapsed: 0:13:02. Loss: 0.42214\n",
            "  Batch 1,650  of  2,031.    Elapsed: 0:13:05. Loss: 0.42214\n",
            "  Batch 1,655  of  2,031.    Elapsed: 0:13:07. Loss: 0.42214\n",
            "  Batch 1,660  of  2,031.    Elapsed: 0:13:10. Loss: 0.42204\n",
            "  Batch 1,665  of  2,031.    Elapsed: 0:13:12. Loss: 0.42202\n",
            "  Batch 1,670  of  2,031.    Elapsed: 0:13:14. Loss: 0.42216\n",
            "  Batch 1,675  of  2,031.    Elapsed: 0:13:17. Loss: 0.42201\n",
            "  Batch 1,680  of  2,031.    Elapsed: 0:13:19. Loss: 0.42198\n",
            "  Batch 1,685  of  2,031.    Elapsed: 0:13:21. Loss: 0.42191\n",
            "  Batch 1,690  of  2,031.    Elapsed: 0:13:24. Loss: 0.42174\n",
            "  Batch 1,695  of  2,031.    Elapsed: 0:13:26. Loss: 0.42183\n",
            "  Batch 1,700  of  2,031.    Elapsed: 0:13:29. Loss: 0.42173\n",
            "  Batch 1,705  of  2,031.    Elapsed: 0:13:31. Loss: 0.42170\n",
            "  Batch 1,710  of  2,031.    Elapsed: 0:13:33. Loss: 0.42165\n",
            "  Batch 1,715  of  2,031.    Elapsed: 0:13:36. Loss: 0.42185\n",
            "  Batch 1,720  of  2,031.    Elapsed: 0:13:38. Loss: 0.42174\n",
            "  Batch 1,725  of  2,031.    Elapsed: 0:13:40. Loss: 0.42172\n",
            "  Batch 1,730  of  2,031.    Elapsed: 0:13:43. Loss: 0.42161\n",
            "  Batch 1,735  of  2,031.    Elapsed: 0:13:45. Loss: 0.42157\n",
            "  Batch 1,740  of  2,031.    Elapsed: 0:13:48. Loss: 0.42163\n",
            "  Batch 1,745  of  2,031.    Elapsed: 0:13:50. Loss: 0.42151\n",
            "  Batch 1,750  of  2,031.    Elapsed: 0:13:52. Loss: 0.42133\n",
            "  Batch 1,755  of  2,031.    Elapsed: 0:13:55. Loss: 0.42149\n",
            "  Batch 1,760  of  2,031.    Elapsed: 0:13:57. Loss: 0.42134\n",
            "  Batch 1,765  of  2,031.    Elapsed: 0:13:59. Loss: 0.42134\n",
            "  Batch 1,770  of  2,031.    Elapsed: 0:14:02. Loss: 0.42143\n",
            "  Batch 1,775  of  2,031.    Elapsed: 0:14:04. Loss: 0.42139\n",
            "  Batch 1,780  of  2,031.    Elapsed: 0:14:07. Loss: 0.42156\n",
            "  Batch 1,785  of  2,031.    Elapsed: 0:14:09. Loss: 0.42150\n",
            "  Batch 1,790  of  2,031.    Elapsed: 0:14:11. Loss: 0.42157\n",
            "  Batch 1,795  of  2,031.    Elapsed: 0:14:14. Loss: 0.42148\n",
            "  Batch 1,800  of  2,031.    Elapsed: 0:14:16. Loss: 0.42134\n",
            "  Batch 1,805  of  2,031.    Elapsed: 0:14:18. Loss: 0.42140\n",
            "  Batch 1,810  of  2,031.    Elapsed: 0:14:21. Loss: 0.42137\n",
            "  Batch 1,815  of  2,031.    Elapsed: 0:14:23. Loss: 0.42134\n",
            "  Batch 1,820  of  2,031.    Elapsed: 0:14:26. Loss: 0.42139\n",
            "  Batch 1,825  of  2,031.    Elapsed: 0:14:28. Loss: 0.42123\n",
            "  Batch 1,830  of  2,031.    Elapsed: 0:14:30. Loss: 0.42107\n",
            "  Batch 1,835  of  2,031.    Elapsed: 0:14:33. Loss: 0.42099\n",
            "  Batch 1,840  of  2,031.    Elapsed: 0:14:35. Loss: 0.42105\n",
            "  Batch 1,845  of  2,031.    Elapsed: 0:14:37. Loss: 0.42093\n",
            "  Batch 1,850  of  2,031.    Elapsed: 0:14:40. Loss: 0.42086\n",
            "  Batch 1,855  of  2,031.    Elapsed: 0:14:42. Loss: 0.42104\n",
            "  Batch 1,860  of  2,031.    Elapsed: 0:14:45. Loss: 0.42089\n",
            "  Batch 1,865  of  2,031.    Elapsed: 0:14:47. Loss: 0.42071\n",
            "  Batch 1,870  of  2,031.    Elapsed: 0:14:49. Loss: 0.42085\n",
            "  Batch 1,875  of  2,031.    Elapsed: 0:14:52. Loss: 0.42074\n",
            "  Batch 1,880  of  2,031.    Elapsed: 0:14:54. Loss: 0.42074\n",
            "  Batch 1,885  of  2,031.    Elapsed: 0:14:57. Loss: 0.42068\n",
            "  Batch 1,890  of  2,031.    Elapsed: 0:14:59. Loss: 0.42061\n",
            "  Batch 1,895  of  2,031.    Elapsed: 0:15:01. Loss: 0.42046\n",
            "  Batch 1,900  of  2,031.    Elapsed: 0:15:04. Loss: 0.42062\n",
            "  Batch 1,905  of  2,031.    Elapsed: 0:15:06. Loss: 0.42058\n",
            "  Batch 1,910  of  2,031.    Elapsed: 0:15:08. Loss: 0.42055\n",
            "  Batch 1,915  of  2,031.    Elapsed: 0:15:11. Loss: 0.42044\n",
            "  Batch 1,920  of  2,031.    Elapsed: 0:15:13. Loss: 0.42048\n",
            "  Batch 1,925  of  2,031.    Elapsed: 0:15:16. Loss: 0.42053\n",
            "  Batch 1,930  of  2,031.    Elapsed: 0:15:18. Loss: 0.42046\n",
            "  Batch 1,935  of  2,031.    Elapsed: 0:15:20. Loss: 0.42049\n",
            "  Batch 1,940  of  2,031.    Elapsed: 0:15:23. Loss: 0.42041\n",
            "  Batch 1,945  of  2,031.    Elapsed: 0:15:25. Loss: 0.42043\n",
            "  Batch 1,950  of  2,031.    Elapsed: 0:15:27. Loss: 0.42048\n",
            "  Batch 1,955  of  2,031.    Elapsed: 0:15:30. Loss: 0.42031\n",
            "  Batch 1,960  of  2,031.    Elapsed: 0:15:32. Loss: 0.42028\n",
            "  Batch 1,965  of  2,031.    Elapsed: 0:15:35. Loss: 0.42005\n",
            "  Batch 1,970  of  2,031.    Elapsed: 0:15:37. Loss: 0.42009\n",
            "  Batch 1,975  of  2,031.    Elapsed: 0:15:39. Loss: 0.42011\n",
            "  Batch 1,980  of  2,031.    Elapsed: 0:15:42. Loss: 0.41995\n",
            "  Batch 1,985  of  2,031.    Elapsed: 0:15:44. Loss: 0.41981\n",
            "  Batch 1,990  of  2,031.    Elapsed: 0:15:46. Loss: 0.41987\n",
            "  Batch 1,995  of  2,031.    Elapsed: 0:15:49. Loss: 0.42007\n",
            "  Batch 2,000  of  2,031.    Elapsed: 0:15:51. Loss: 0.42004\n",
            "  Batch 2,005  of  2,031.    Elapsed: 0:15:54. Loss: 0.42007\n",
            "  Batch 2,010  of  2,031.    Elapsed: 0:15:56. Loss: 0.42001\n",
            "  Batch 2,015  of  2,031.    Elapsed: 0:15:58. Loss: 0.41984\n",
            "  Batch 2,020  of  2,031.    Elapsed: 0:16:01. Loss: 0.41982\n",
            "  Batch 2,025  of  2,031.    Elapsed: 0:16:03. Loss: 0.41985\n",
            "  Batch 2,030  of  2,031.    Elapsed: 0:16:06. Loss: 0.41965\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epcoh took: 0:16:06\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.40\n",
            "  Validation took: 0:01:10\n",
            "  Accuracy: 0.88\n",
            "  Accuracy: 0.92\n",
            "  Accuracy: 0.93\n",
            "  Accuracy: 0.85\n",
            "  Accuracy: 0.57\n",
            "  Macro F1-score: 0.47\n",
            "  Macro F1-score: 0.48\n",
            "  Macro F1-score: 0.48\n",
            "  Macro F1-score: 0.46\n",
            "  Macro F1-score: 0.37\n",
            "  Weighted F1-score: 0.82\n",
            "  Weighted F1-score: 0.89\n",
            "  Weighted F1-score: 0.89\n",
            "  Weighted F1-score: 0.78\n",
            "  Weighted F1-score: 0.42\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      1.00      0.94      3570\n",
            "         1.0       0.00      0.00      0.00       491\n",
            "\n",
            "    accuracy                           0.88      4061\n",
            "   macro avg       0.44      0.50      0.47      4061\n",
            "weighted avg       0.77      0.88      0.82      4061\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      1.00      0.96      3751\n",
            "         1.0       0.00      0.00      0.00       310\n",
            "\n",
            "    accuracy                           0.92      4061\n",
            "   macro avg       0.46      0.50      0.48      4061\n",
            "weighted avg       0.85      0.92      0.89      4061\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      1.00      0.96      3762\n",
            "         1.0       0.00      0.00      0.00       299\n",
            "\n",
            "    accuracy                           0.93      4061\n",
            "   macro avg       0.46      0.50      0.48      4061\n",
            "weighted avg       0.86      0.93      0.89      4061\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      1.00      0.92      3435\n",
            "         1.0       0.00      0.00      0.00       626\n",
            "\n",
            "    accuracy                           0.85      4061\n",
            "   macro avg       0.42      0.50      0.46      4061\n",
            "weighted avg       0.72      0.85      0.78      4061\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00      1726\n",
            "         1.0       0.57      1.00      0.73      2335\n",
            "\n",
            "    accuracy                           0.57      4061\n",
            "   macro avg       0.29      0.50      0.37      4061\n",
            "weighted avg       0.33      0.57      0.42      4061\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3570    0]\n",
            " [ 491    0]]\n",
            "[[3751    0]\n",
            " [ 310    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mj6kkW4KFvNq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}